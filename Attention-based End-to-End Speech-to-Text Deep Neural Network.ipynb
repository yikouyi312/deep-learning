{"cells":[{"cell_type":"markdown","metadata":{"id":"I8a_cZqr-UpV"},"source":["# HW4P2: Attention-based Speech Recognition\n","\n","Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with attention. <br> <br>\n","\n","HW Writeup: https://piazza.com/class_profile/get_resource/l37uyxe87cq5xn/lam1lcjjj0314e <br>\n","Kaggle competition link: https://www.kaggle.com/competitions/11-785-f22-hw4p2/ <br>\n","LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n","Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"]},{"cell_type":"markdown","metadata":{"id":"Vlev_Tvq_bRz"},"source":["# Initial Set-up"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1671077862965,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"0pueIzbxUwyY","outputId":"a4c9df64-7f7b-4d92-f46a-d47d8b69883a"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tepaid_TDwWt"},"outputs":[],"source":["# Install some required libraries\n","# Feel free to add more if you want\n","!pip install -q python-levenshtein torchsummaryX wandb"]},{"cell_type":"markdown","metadata":{"id":"lr4xGzRU-KZz"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2144,"status":"ok","timestamp":1671077872492,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"ZectxKF3XEVV","outputId":"fee2a807-4ae3-4800-e4c6-a2c81ef960ab"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import Levenshtein\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import torchaudio.transforms as tat\n","from sklearn.metrics import accuracy_score\n","import gc\n","import math\n","import zipfile\n","from tqdm import tqdm\n","import datetime\n","\n","import torch\n","import torchaudio\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","import gc\n","from torchsummaryX import summary\n","import wandb\n","from glob import glob\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device: \", DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5HLepZpA5WS"},"outputs":[],"source":["# TODO: Import drive if you are a colab user\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669491249483,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"yQ95umTcAOYC","outputId":"9a0d36a8-c312-4517-bef7-326137f35852"},"outputs":[],"source":["%cd /content/drive/MyDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FoDi9EtpADYw"},"outputs":[],"source":["!unzip -qo 'hw4p2.zip' -d '/content/drive/MyDrive/data'"]},{"cell_type":"markdown","metadata":{"id":"OALQCI0EDCwh"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqDuibMCP345"},"outputs":[],"source":["# Global config dict. Feel free to add or change if you want.\n","config = {\n","    'batch_size': 96,\n","    'epochs': 30,\n","    'lr': 1e-3\n","}"]},{"cell_type":"markdown","metadata":{"id":"X7J4sY1OW9Pr"},"source":["# Toy Data Setup"]},{"cell_type":"markdown","metadata":{"id":"JTsQB-pvRLLs"},"source":["The toy dataset is very essential for you in this HW. The model which you will be building is complicated and you first need to make sure that it runs on the toy dataset. <br>\n","In other words, you need convergence - the attention diagonal. Take a look at the write-up for this. <br>\n","We have given you the following code to download the toy data and load it. You can use it the way it is. But be careful, the transcripts are different from the original data from kaggle. The toy dataset has phonemes but the actual data has characters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32056,"status":"ok","timestamp":1669492830206,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"Des4AMIaW4E8","outputId":"f7d36d5d-509e-4fc6-9ce3-443dd3022c3d"},"outputs":[],"source":["!wget -q https://cmu.box.com/shared/static/wok08c2z2dp4clufhy79c5ee6jx3pyj9 --content-disposition --show-progress\n","!wget -q https://cmu.box.com/shared/static/zctr6mvh7npfn01forli8n45duhp2g85 --content-disposition --show-progress\n","!wget -q https://cmu.box.com/shared/static/m2oaek69145ljeu6srtbbb7k0ip6yfup --content-disposition --show-progress\n","!wget -q https://cmu.box.com/shared/static/owrjy0tqra3v7zq2ru7mocy2djskydy9 --content-disposition --show-progress"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fLveDeiXCsb"},"outputs":[],"source":["# Load the toy dataset\n","X_train = np.load(\"f0176_mfccs_train.npy\")\n","X_valid = np.load(\"f0176_mfccs_dev.npy\")\n","Y_train = np.load(\"f0176_hw3p2_train.npy\")\n","Y_valid = np.load(\"f0176_hw3p2_dev.npy\")\n","\n","# This is how you actually need to find out the different trancripts in a dataset. \n","# Can you think whats going on here? Why are we using a np.unique?\n","VOCAB_MAP           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n","VOCAB_MAP[\"[PAD]\"]  = len(VOCAB_MAP)\n","VOCAB               = list(VOCAB_MAP.keys())\n","\n","SOS_TOKEN = VOCAB_MAP[\"[SOS]\"]\n","EOS_TOKEN = VOCAB_MAP[\"[EOS]\"]\n","PAD_TOKEN = VOCAB_MAP[\"[PAD]\"]\n","\n","Y_train = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_train]\n","Y_valid = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_valid]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNjjsFqpbocR"},"outputs":[],"source":["# Dataset class for the Toy dataset\n","class ToyDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, partition):\n","\n","        if partition == \"train\":\n","            self.mfccs = X_train[:, :, :15]\n","            self.transcripts = Y_train\n","\n","        elif partition == \"valid\":\n","            self.mfccs = X_valid[:, :, :15]\n","            self.transcripts = Y_valid\n","\n","        assert len(self.mfccs) == len(self.transcripts)\n","\n","        self.length = len(self.mfccs)\n","\n","    def __len__(self):\n","\n","        return self.length\n","\n","    def __getitem__(self, i):\n","\n","        x = torch.tensor(self.mfccs[i])\n","        y = torch.tensor(self.transcripts[i])\n","\n","        return x, y\n","\n","    def collate_fn(batch):\n","        x_batch, y_batch = list(zip(*batch))\n","        x_lens      = [x.shape[0] for x in x_batch] \n","        y_lens      = [y.shape[0] for y in y_batch] \n","        x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN)\n","        y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN) \n","        \n","        return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TI97IFWzFxQM"},"outputs":[],"source":["train_data = ToyDataset(partition= \"train\")\n","train_loader = torch.utils.data.DataLoader(train_data, \n","                                           collate_fn=ToyDataset.collate_fn,\n","                                           num_workers= 4, \n","                                           batch_size=config['batch_size'], \n","                                           pin_memory= True,shuffle=True\n","                                           )\n","for data in train_loader:\n","    x_batch_pad, y_batch_pad, x_len, y_len = data\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669492835013,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"e8c3uSqwHI2o","outputId":"f2cfd0fc-13da-45f0-a895-8c35b2462e95"},"outputs":[],"source":["print(X_train.shape)\n","print(len(Y_train))\n","print(x_batch_pad.shape)\n","print(x_len.shape)\n","print(y_batch_pad.shape)\n","print(y_len.shape)\n","print(x_len)\n","print(y_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1eRepLGT1W2"},"outputs":[],"source":["example_batch = x_batch_pad\n","example_len = x_len"]},{"cell_type":"markdown","metadata":{"id":"YWRjucnUdbQ1"},"source":["# Kaggle Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2295,"status":"ok","timestamp":1670091565479,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"fStTuAQ6XAuD","outputId":"5871c98e-4a4a-41b2-ccff-697f047ee238"},"outputs":[],"source":["# TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n","!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle/\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"\",\"key\":\"\"}') # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39570,"status":"ok","timestamp":1670091605043,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"nR74ooCSa664","outputId":"47a167e0-5442-40c7-8cd0-73b00ccf897f"},"outputs":[],"source":["# Download the data\n","!kaggle competitions download -c 11-785-f22-hw4p2\n","!mkdir '/content/data'\n","\n","!unzip -qo '11-785-f22-hw4p2.zip' -d '/content/data'"]},{"cell_type":"markdown","metadata":{"id":"i5ioyn6ldQB9"},"source":["# Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1671158523645,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"YHix8UvpBWh7"},"outputs":[],"source":["# These are the various characters in the transcripts of the datasetW\n","VOCAB = ['<sos>',   \n","         'A',   'B',    'C',    'D',    \n","         'E',   'F',    'G',    'H',    \n","         'I',   'J',    'K',    'L',       \n","         'M',   'N',    'O',    'P',    \n","         'Q',   'R',    'S',    'T', \n","         'U',   'V',    'W',    'X', \n","         'Y',   'Z',    \"'\",    ' ', \n","         '<eos>']\n","\n","VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n","SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n","EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n","padding = EOS_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1671158523876,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"ldJiyo7-pYLr"},"outputs":[],"source":["# TODO: Create a dataset class which is exactly the same as HW3P2. You are free to reuse it. \n","# The only change is that the transcript mapping is different for this HW.\n","# Note: We also want to retain SOS and EOS tokens in the transcript this time.\n","class AudioDataset(torch.utils.data.Dataset):\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","    #TODO\n","    def __init__(self, data_path, partition= \"train-clean-100\", train = True, limit=-1, time_mask_param = 100, freq_mask_param = 6): \n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        ''' \n","        timemask = torchaudio.transforms.TimeMasking(time_mask_param=time_mask_param)\n","        freqmask =  torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask_param)\n","        self.transform = nn.Sequential(timemask, freqmask)\n","        self.train = train\n","        # Load the directory and all files in them\n","        self.data_path = data_path\n","\n","        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' #TODO\n","        self.transcript_dir = self.data_path +'/'+ partition + '/transcript/raw' #TODO\n","\n","        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n","        self.transcript_files = sorted(os.listdir(self.transcript_dir)) #TODO\n","\n","        assert len(self.mfcc_files) == len(self.transcript_files) \n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfcc_files)\n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        VOCAB = ['<sos>',   \n","         'A',   'B',    'C',    'D',    \n","         'E',   'F',    'G',    'H',    \n","         'I',   'J',    'K',    'L',       \n","         'M',   'N',    'O',    'P',    \n","         'Q',   'R',    'S',    'T', \n","         'U',   'V',    'W',    'X', \n","         'Y',   'Z',    \"'\",    ' ', \n","         '<eos>']\n","        VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n","        SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n","        EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n","        self.padding = EOS_TOKEN\n","        self.PHONEMES = VOCAB\n","        self.mapping = VOCAB_MAP\n","        #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        self.mfccs, self.transcripts = [], []\n","        for i in range(0, len(self.mfcc_files)):\n","            mfcc = np.load(self.mfcc_dir + '/' + self.mfcc_files[i])\n","        #   Gaussin Normalization of mfcc\n","            mean = np.mean(mfcc,axis = 0)\n","            sigma = np.std(mfcc, axis = 0)\n","            mfcc = (mfcc - mean)/sigma\n","        #   Load the corresponding transcript\n","        #   Remove [SOS] and [EOS] from the transcript \n","            transcript = np.load(self.transcript_dir + '/' + self.transcript_files[i])\n","            transcript = transcript\n","            # Map to label\n","            transcript = [self.mapping[transcript[index]] for index in range(len(transcript))]\n","            # transcript = [self.LABELS.index(self.mapping[transcript[index]]) for index in range(len(transcript))]\n","        #   Append each mfcc to self.mfcc, transcript to self.transcript\n","            self.mfccs.append(mfcc)\n","            self.transcripts.append(transcript)\n","            # self.label.append(label)\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","       \n","    def __len__(self):\n","        \n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","        Once done, return a tuple of features and labels.\n","        '''\n","        mfcc = torch.Tensor(self.mfccs[ind])# TODO\n","        if self.train:\n","            freqmask =  torchaudio.transforms.FrequencyMasking(freq_mask_param= 6)\n","            timemask = torchaudio.transforms.TimeMasking(time_mask_param=int(mfcc.size(0)*0.25))\n","            mfcc = mfcc.transpose(0,1)\n","            mfcc = mfcc.unsqueeze(0)\n","            mfcc = timemask(mfcc)\n","            mfcc = freqmask(mfcc)\n","            mfcc = mfcc.squeeze(0)\n","            mfcc = mfcc.transpose(0,1)\n","        transcript = torch.LongTensor(self.transcripts[ind])# TODO\n","\n","        return mfcc, transcript\n","\n","    def collate_fn(batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish. \n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features, \n","            and lengths of labels.\n","        '''\n","        # batch of input mfcc coefficients\n","        # batch of output phonemes\n","        batch_mfcc, batch_transcript = zip(*batch)# TODO\n","        #batch_transcript = [transcript for mfcc, transcript in batch] # TODO\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value= padding) # TODO\n","        lengths_mfcc =  torch.LongTensor([len(x) for x in batch_mfcc])# TODO \n","\n","        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value= padding) # TODO\n","        lengths_transcript =  torch.LongTensor([len(x) for x in batch_transcript])# TODO\n","\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","        \n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671158523876,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"zannKblcPORG"},"outputs":[],"source":["# TODO: Similarly, create a test dataset class\n","# Test Dataloader\n","#TODO\n","class AudioDatasetTest(torch.utils.data.Dataset):\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","    #TODO\n","    def __init__(self, data_path, partition= \"test-clean\", limit=-1): \n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        ''' \n","        # Load the directory and all files in them\n","        self.data_path = data_path\n","        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' #TODO\n","        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = len(self.mfcc_files)\n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        VOCAB = ['<sos>',   \n","         'A',   'B',    'C',    'D',    \n","         'E',   'F',    'G',    'H',    \n","         'I',   'J',    'K',    'L',       \n","         'M',   'N',    'O',    'P',    \n","         'Q',   'R',    'S',    'T', \n","         'U',   'V',    'W',    'X', \n","         'Y',   'Z',    \"'\",    ' ', \n","         '<eos>']\n","        VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n","        SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n","        EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n","        self.PHONEMES = VOCAB\n","        self.mapping = VOCAB_MAP\n","        #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        self.mfccs = []\n","        for i in range(0, len(self.mfcc_files)):\n","            mfcc = np.load(self.mfcc_dir + '/' + self.mfcc_files[i])\n","        #   Gaussin Normalization of mfcc\n","            mean = np.mean(mfcc,axis = 0)\n","            sigma = np.std(mfcc, axis = 0)\n","            mfcc = (mfcc - mean)/sigma\n","        #   Append each mfcc to self.mfcc\n","            self.mfccs.append(mfcc)\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","       \n","    def __len__(self):\n","        \n","        '''\n","        TODO: What do we return here?\n","        '''\n","        return self.length\n","        # raise NotImplemented\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","        Once done, return a tuple of features and labels.\n","        '''\n","        mfcc = torch.Tensor(self.mfccs[ind])# TODO\n","        return mfcc\n","\n","    def collate_fn(batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish. \n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features, \n","            and lengths of labels.\n","        '''\n","        # batch of input mfcc coefficients\n","        # batch of output phonemes\n","        batch_mfcc = batch # TODO\n","        #batch_transcript = [transcript for mfcc, transcript in batch] # TODO\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=padding) # TODO\n","        lengths_mfcc =  torch.LongTensor([len(x) for x in batch_mfcc]) # TODO \n","        \n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"]},{"cell_type":"markdown","metadata":{"id":"gQenneVsDLnX"},"source":["# Dataset and Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1671158524820,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"uB5mkdUIE4bM"},"outputs":[],"source":["root = '/content/data/hw4p2' "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":54758,"status":"ok","timestamp":1671158579690,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"oi2FS_hkDQZB"},"outputs":[],"source":["# TODO: Create the datasets and dataloaders\n","# All these things are similar to HW3P2\n","# You can reuse the same code\n","# Create objects for the dataset class\n","train_data = AudioDataset(root, partition= \"train-clean-100\", train = True, limit=-1) #TODO\n","train_data1 = AudioDataset(root, partition= \"train-clean-100\", train = False, limit=-1)\n","val_data = AudioDataset(root, partition= \"dev-clean\", train = False, limit=-1) # TODO : You can either use the same class with some modifications or make a new one :)\n","test_data = AudioDatasetTest(root, partition= \"test-clean\", limit=-1) #TODO\n","\n","# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n","train_loader = torch.utils.data.DataLoader(train_data, \n","                                           collate_fn=AudioDataset.collate_fn,\n","                                           num_workers= 4, \n","                                           batch_size=config['batch_size'], \n","                                           pin_memory= True,shuffle=True\n","                                           )\n","train_loader1 = torch.utils.data.DataLoader(train_data1, \n","                                           collate_fn=AudioDataset.collate_fn,\n","                                           num_workers= 4, \n","                                           batch_size=config['batch_size'], \n","                                           pin_memory= True,shuffle=True\n","                                           )\n","val_loader = torch.utils.data.DataLoader(val_data, \n","                                         collate_fn=AudioDataset.collate_fn,\n","                                         num_workers= 4, \n","                                         batch_size=config['batch_size'], \n","                                         pin_memory= True,shuffle=False\n","                                         )#TODO\n","test_loader = torch.utils.data.DataLoader(test_data, \n","                                          collate_fn=AudioDatasetTest.collate_fn,\n","                                          num_workers= 4, \n","                                          batch_size=config['batch_size'], \n","                                          pin_memory= True,shuffle=False\n","                                          )#TODO\n","# The sanity check for shapes also are similar\n","# Please remember that the only change in the dataset for this HW is the transcripts\n","# So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1671158581770,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"Xoboi6i1I_oO"},"outputs":[],"source":["for data in train_loader:\n","    x_batch_pad, y_batch_pad, x_len, y_len = data\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671158581770,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"e5XX2zb3JIiz","outputId":"2a761e4b-cfa7-4498-864b-d4db26567075"},"outputs":[],"source":["print(padding)\n","print(x_batch_pad.shape) # batch_size x length x dim, B x T x D\n","print(max(x_len))\n","print(x_len.shape)\n","print(y_batch_pad.shape) # batch_size x length \n","print(max(y_len))\n","print(y_len.shape)\n","print(x_len)\n","print(y_len)\n","print(x_batch_pad[0, :, 0].shape)\n","print(y_batch_pad[0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581770,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"4n6tb4HtLd0g"},"outputs":[],"source":["packed_x = pack_padded_sequence(x_batch_pad, x_len, batch_first = True,  enforce_sorted=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581770,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"UJDKyqFnLlIJ","outputId":"30ae575c-77b3-4bae-d77a-da4becf65f26"},"outputs":[],"source":["print(packed_x)\n","print(packed_x.data.shape)\n","print(packed_x.batch_sizes.shape)"]},{"cell_type":"markdown","metadata":{"id":"I--VjKlEhwi8"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"uql9E6cqROvJ"},"source":["In this section you will be building the LAS model from scratch. Before starting to code, please read the writeup, paper and understand the following parts completely.<br>\n","- Pyramidal Bi-LSTM \n","- Listener\n","- Attention\n","- Speller\n","\n","After getting a good grasp of the workings of these modules, start coding. Follow the TODOs carefully. We will also be adding some extra features to the attention mechanism like keys and values which are not originally present in LAS. So we will be creating a hybrid network based on LAS and Attention is All You Need.\n"]},{"cell_type":"markdown","metadata":{"id":"lCbwz0LZMWwe"},"source":["## Encoder"]},{"cell_type":"markdown","metadata":{"id":"GuLso2WWOlcA"},"source":["### LockedDrop"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"KfvKqDj6OnyS"},"outputs":[],"source":["class LockedDropout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, dropout=0.5):\n","        if not self.training or not dropout:\n","            return x\n","        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - dropout)\n","        mask = mask.div_(1 - dropout)\n","        mask = mask.expand_as(x)\n","        return mask * x"]},{"cell_type":"markdown","metadata":{"id":"jOggaRhTMHhA"},"source":["#### ResidualBlock\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"o-RlXGBPy2ud"},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels = 15, out_channels = 64, stride = 1, dropout = 0.0, downsample = None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Sequential(\n","                        nn.ConstantPad1d(3 // 2, 0),\n","                        nn.Conv1d(in_channels, out_channels, kernel_size = 3, stride = stride, bias = False),\n","                        nn.BatchNorm1d(out_channels),\n","                        nn.ReLU(),\n","                        torch.nn.Dropout(dropout),\n","\n","                        nn.ConstantPad1d(3 // 2, 0),\n","                        nn.Conv1d(out_channels, out_channels, kernel_size = 3, stride = 1, bias = False),\n","                        nn.BatchNorm1d(out_channels),\n","                        torch.nn.Dropout(dropout)\n","                        )\n","\n","        self.downsample = downsample\n","        self.relu = nn.ReLU()\n","        self.out_channels = out_channels\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"XoI0zEoIMX5I"},"source":["### Pyramidal Bi-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"-F9zAQR95P55"},"outputs":[],"source":["class pBLSTM(torch.nn.Module):\n","    '''\n","    Pyramidal BiLSTM\n","    Read the write up/paper and understand the concepts and then write your implementation here.\n","\n","    At each step,\n","    1. Pad your input if it is packed (Unpack it)\n","    2. Reduce the input length dimension by concatenating feature dimension\n","        (Tip: Write down the shapes and understand)\n","        (i) How should  you deal with odd/even length input? \n","        (ii) How should you deal with input length array (x_lens) after truncating the input?\n","    3. Pack your input\n","    4. Pass it into LSTM layer\n","    To make our implementation modular, we pass 1 layer at a time.\n","    '''\n","    def __init__(self, input_size, hidden_size):\n","        super(pBLSTM, self).__init__()\n","        self.downsamples = 2\n","        self.input_size = input_size\n","        self.blstm = nn.LSTM(input_size = input_size * self.downsamples, \n","                             hidden_size = hidden_size, \n","                             num_layers = 1, \n","                             batch_first = True, \n","                             bidirectional = True) \n","       # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n","\n","    def forward(self, x_packed): # x_packed is a PackedSequence\n","        # TODO: Pad Packed Sequence\n","        x, x_len = pad_packed_sequence(x_packed, batch_first=True)\n","        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n","        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n","        x, x_len = self.trunc_reshape(x, x_len)\n","        # TODO: Pack Padded Sequence. What output(s) would you get?\n","        x_packed = pack_padded_sequence(x, x_len, batch_first = True,  enforce_sorted=False)\n","        # TODO: Pass the sequence through bLSTM\n","        packed_out = self.blstm(x_packed)[0]\n","        return packed_out\n","\n","    def trunc_reshape(self, x, x_lens): \n","        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n","        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n","        # TODO: Reduce lengths by the same downsampling factor\n","        batch_size = x.size(0)\n","        length = x.size(1)\n","        feature = x.size(2)\n","        if length % self.downsamples != 0:\n","          reminder =  int(length % self.downsamples)\n","          x = x[:, :-reminder, :]\n","          length -= reminder\n","          # x_lens -= reminder\n","        x = x.view((batch_size, length // self.downsamples, feature * self.downsamples))\n","        x_lens = x_lens // self.downsamples #+ x_lens % self.downsamples\n","        return x, x_lens"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"mTfKOPABQs63"},"outputs":[],"source":["class pBLSTMwithLockdrop(torch.nn.Module):\n","    '''\n","    '''\n","    def __init__(self, encoder_hidden_size, drop):\n","        super(pBLSTMwithLockdrop, self).__init__()\n","        self.blstm1 = pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1* encoder_hidden_size)\n","        self.blstm2 = pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1* encoder_hidden_size)\n","        self.blstm3 = pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1* encoder_hidden_size)\n","        self.blstm4 = pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1* encoder_hidden_size)\n","        self.drop1 = LockedDropout()\n","        self.drop2 = LockedDropout()\n","        self.drop3 = LockedDropout()\n","        self.drop4 = LockedDropout()\n","        self.p = drop\n","\n","       # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n","\n","    def forward(self, x_packed): # x_packed is a PackedSequence\n","        x_packed = self.blstm1(x_packed)\n","        x_outputs, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        x_outputs = self.drop1(x_outputs, dropout=self.p[0])\n","        x_packed = pack_padded_sequence(x_outputs, x_lens, batch_first = True,  enforce_sorted=False)\n","\n","        x_packed = self.blstm2(x_packed)\n","        x_outputs, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        x_outputs = self.drop2(x_outputs, dropout=self.p[1])\n","        x_packed = pack_padded_sequence(x_outputs, x_lens, batch_first = True,  enforce_sorted=False)\n","        \n","        x_packed = self.blstm3(x_packed)\n","        x_outputs, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        x_outputs = self.drop3(x_outputs, dropout=self.p[2])\n","        x_packed = pack_padded_sequence(x_outputs, x_lens, batch_first = True,  enforce_sorted=False)\n","        \n","        x_packed = self.blstm4(x_packed)\n","        x_outputs, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        x_outputs = self.drop4(x_outputs, dropout=self.p[3])\n","        x_packed = pack_padded_sequence(x_outputs, x_lens, batch_first = True,  enforce_sorted=False)\n","        \n","        return x_packed"]},{"cell_type":"markdown","metadata":{"id":"_rchbyjlMeB2"},"source":["### Listener"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"912b3sVoHr1e"},"outputs":[],"source":["class Listener(torch.nn.Module):\n","    '''\n","    The Encoder takes utterances as inputs and returns latent feature representations\n","    '''\n","    def __init__(self, input_size, cnn, encoder_hidden_size, layer, cnn_drop, lstm_drop):\n","        super(Listener, self).__init__()\n","        #[64, 128]\n","        self.cnn_outputsize = cnn[-1]\n","        self.down_sample = 2\n","        # self.cnn = torch.nn.Conv1d(input_size, self.cnn_outputsize, kernel_size=1, stride=1)\n","        self.cnn1 = self.make_layer(input_size, cnn[0], block_num = layer[0], stride = 1, drop = cnn_drop)\n","        self.cnn2 = self.make_layer(cnn[0], cnn[1], block_num = layer[1], stride = 1, drop = cnn_drop)\n","        # self.cnn3 = self.make_layer(cnn[1], cnn[2], block_num = layer[2], stride = 1, drop = cnn_drop)\n","        # The first LSTM at the very bottom\n","        self.base_lstm = torch.nn.LSTM(input_size = self.cnn_outputsize, \n","                                       hidden_size = encoder_hidden_size, \n","                                       num_layers = 1, \n","                                       batch_first = True, \n","                                       bidirectional = True)#TODO: Fill this up\n","\n","        # self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n","        #     # TODO: Fill this up with pBLSTMs - What should the input_size be? \n","        #     pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1* encoder_hidden_size),\n","        #     LockedDropout(),\n","        #     pBLSTM(input_size = 2* encoder_hidden_size, hidden_size= 1* encoder_hidden_size),\n","        #     LockedDropout(),\n","        #     pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1 * encoder_hidden_size),\n","        #     LockedDropout(),\n","        #     pBLSTM(input_size = 2 * encoder_hidden_size, hidden_size= 1 * encoder_hidden_size),\n","        #     LockedDropout(),\n","        #     # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n","        #     # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n","        #     # ...\n","        #     # ...\n","        # )\n","        self.pBLSTMs = pBLSTMwithLockdrop(encoder_hidden_size = encoder_hidden_size, drop = lstm_drop)\n","    \n","    def make_layer(self, in_channel, out_channel, block_num, stride=1, drop=0):\n","        downsample = None\n","        if stride != 1 or in_channel != out_channel:\n","            downsample = nn.Sequential(\n","                 nn.Conv1d(in_channel, out_channel, kernel_size=1, stride=1, bias = False),\n","                 nn.BatchNorm1d(out_channel),\n","            )\n","        layers = []\n","        layers.append(ResidualBlock(in_channel, out_channel, stride, drop, downsample))\n","        in_channel = out_channel\n","        for i in range(1, block_num):\n","            layers.append(ResidualBlock(in_channel, out_channel, stride, drop))\n","\n","        return nn.Sequential(*layers)\n","         \n","    def forward(self, x, x_lens):\n","        # Where are x and x_lens coming from? The dataloader\n","        # x : B x L x D---> B x D x L --> B x cnn_outputsize x L --> B x L x cnn_outputsize\n","        x = self.cnn1(x.transpose(1,2))\n","        x = self.cnn2(x)\n","        # x = self.cnn3(x)\n","        x = x.transpose(1,2)\n","        # TODO: Pack Padded Sequence\n","        x_packed = pack_padded_sequence(x, x_lens, batch_first = True,  enforce_sorted=False)\n","        # TODO: Pass it through the first LSTM layer (no truncation)\n","        x_packed = self.base_lstm(x_packed)[0]\n","        # TODO: Pad Packed Sequence\n","        # x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n","        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n","        encoder_packed = self.pBLSTMs(x_packed)\n","        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_packed, batch_first=True)\n","        # Remember the number of output(s) each function returns\n","        # return encoder_packed\n","        return encoder_outputs, encoder_lens"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671158581771,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"0EgRqsKDI7dD"},"outputs":[],"source":["# example_batch, example_len = x_batch_pad, x_len\n","# encoder = Listener(input_size = 15, cnn=[64, 128], encoder_hidden_size = 256, layer =[2,2], cnn_drop=0).to(DEVICE)# TODO: Initialize Listener\n","# print(encoder)\n","# # summary(encoder, example_batch[0].to(DEVICE), example_batch[3])\n","# summary(encoder, example_batch.to(DEVICE), example_len)\n","# encoder_outputs, encoder_lens = encoder(example_batch.to(DEVICE), example_len)\n","# print(encoder_outputs.shape)\n","# print(encoder_lens.shape)\n","# print(encoder_lens)\n","# del encoder\n","# del example_batch\n","# del example_len"]},{"cell_type":"markdown","metadata":{"id":"JJCpBcEmMVcZ"},"source":["## Attention (Attend)"]},{"cell_type":"markdown","metadata":{"id":"f6k9R7jKMRcZ"},"source":["### Different ways to compute Attention\n","\n","1. Dot-product attention\n","    * raw_weights = bmm(key, query) \n","    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n","    * Check \"Attention is All You Need\" Section 3.2.1\n","    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n","\n","\n","2. Cosine attention\n","    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n","\n","3. Bi-linear attention\n","    * W = Linear transformation (learnable parameter): d_k -> d_q\n","    * raw_weights = bmm(key @ W, query)\n","\n","4. Multi-layer perceptron\n","    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n","\n","5. Multi-Head Attention\n","    * Check \"Attention is All You Need\" Section 3.2.2\n","    * h = Number of heads\n","    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n","    * W_O: d_v -> d_v\n","    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n","    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n","    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n","    * raw_weights = Q @ K^T\n","    * masked_raw_weights = mask(raw_weights)\n","    * attention = softmax(masked_raw_weights)\n","    * multi_head = attention @ V\n","    * multi_head = multi_head reshaped to (B, d_v)\n","    * context = multi_head @ W_O"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671158581898,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"pqu-MUM8TjUO"},"outputs":[],"source":["def plot_attention(attention): \n","    # Function for plotting attention\n","    # You need to get a diagonal plot\n","    plt.clf()\n","    sns.heatmap(attention, cmap='GnBu')\n","    plt.show()\n","\n","class Attention(torch.nn.Module):\n","    '''\n","    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n","    Here are different ways to compute attention and context:\n","    After obtaining the raw weights, compute and return attention weights and context as follows.:\n","    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n","    attention           = softmax(masked_raw_weights)\n","    context             = bmm(attention, value)\n","    At the end, you can pass context through a linear layer too.\n","    '''\n","    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n","        super(Attention, self).__init__()\n","        self.encoder_hidden_size = encoder_hidden_size \n","        self.key_projection     = nn.Linear(2 * encoder_hidden_size, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n","        self.value_projection   = nn.Linear(2 * encoder_hidden_size, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n","        self.query_projection   = nn.Linear(decoder_output_size, projection_size)# TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n","        # Optional : Define an nn.Linear layer which projects the context vector\n","        self.softmax            = nn.Softmax(dim = 1)# TODO: Define a softmax layer. Think about the dimension which you need to apply \n","        # Tip: What is the shape of energy? And what are those?\n","\n","    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n","    # This function is used to calculate them and set them to self\n","    def set_key_value_mask(self, encoder_outputs, encoder_lens): # B x L x D, B\n","    \n","        _, encoder_max_seq_len, _ = encoder_outputs.shape\n","        self.key      = self.key_projection(encoder_outputs)# TODO: Project encoder_outputs using key_projection to get keys, B x L x decoder_output_size\n","        self.value    = self.value_projection(encoder_outputs)# TODO: Project encoder_outputs using value_projection to get values\n","        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n","        # The raw_weights are of shape (batch_size, timesteps)\n","        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n","        # The mask is False for all indicies before padding begins, True for all indices after.\n","        # 1 x L >= B x 1 --> B x L, Ture is padding, False is not padding \n","        self.padding_mask     =  torch.arange(encoder_outputs.size(1)).unsqueeze(0) >= encoder_lens.unsqueeze(1) # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n","        # (Hint: Broadcasting gives you a one liner)\n","        \n","    def forward(self, decoder_output_embedding):\n","        # key   : (batch_size, timesteps, projection_size)\n","        # value : (batch_size, timesteps, projection_size)\n","        # query : (batch_size, projection_size)\n","        # print(self.encoder_hidden_size)\n","        self.query         = self.query_projection(decoder_output_embedding)# TODO: Project the query using query_projection\n","        # Hint: Take a look at torch.bmm for the products below \n","        # B x L x projection_size @ B x projections_size x 1 = B x L x 1 --> B x L\n","        raw_weights        =  torch.bmm(self.key, self.query.unsqueeze(2)).squeeze(2) # * 1 / torch.sqrt(self.query.size(1)) \n","        # print(raw_weights.shape)\n","        # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n","        MASKING_VALUE = -1e+9 if raw_weights.dtype == torch.float32 else -1e+4\n","        # print(raw_weights.dtype)\n","        masked_raw_weights = raw_weights.masked_fill_(self.padding_mask.to(DEVICE), value=MASKING_VALUE)# TODO: Mask the raw_weights with self.padding_mask.\n","        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n","        # B x L \n","        attention_weights  = self.softmax(masked_raw_weights)# TODO: Calculate the attention weights, which is the softmax of raw_weights\n","        # B x 1 x L @ B x L x projection_size = B x 1 x projection_size -- > Bx projection_size\n","        context            = torch.bmm(attention_weights.unsqueeze(1), self.value).squeeze(1)# TODO: Calculate the context - it is a product between attention_weights and value\n","        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n","        return context, attention_weights # Return the context, attention_weights, B x projection_size, B x L"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671158581899,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"7ha4JTPzIxWs"},"outputs":[],"source":["class Attend(torch.nn.Module):\n","    def __init__(self):\n","        super(Attend, self).__init__()\n","        self.softmax            = nn.Softmax(dim = 1)# TODO: Define a softmax layer. Think about the dimension which you need to apply \n","    def forward(self, query, key, value, mask):\n","        # Hint: Take a look at torch.bmm for the products below \n","        # B x L x projection_size @ B x projections_size x 1 = B x L x 1 --> B x L\n","        raw_weights        =  torch.bmm(key, query.unsqueeze(2)).squeeze(2) # * 1 / torch.sqrt(self.query.size(1)) \n","        # print(raw_weights.shape)\n","        # TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n","        MASKING_VALUE = -1e+9 if raw_weights.dtype == torch.float32 else -1e+4\n","        masked_raw_weights = raw_weights.masked_fill_(mask.to(DEVICE), value=MASKING_VALUE)# TODO: Mask the raw_weights with self.padding_mask.\n","        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n","        # B x L \n","        attention_weights  = self.softmax(masked_raw_weights)# TODO: Calculate the attention weights, which is the softmax of raw_weights\n","        # B x 1 x L @ B x L x projection_size = B x 1 x projection_size -- > Bx projection_size\n","        context            = torch.bmm(attention_weights.unsqueeze(1), value).squeeze(1)# TODO: Calculate the context - it is a product between attention_weights and value\n","        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n","        return context, attention_weights # Return the context, attention_weights, B x projection_size, B x L"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671158581899,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"vfwsnZxOA34G"},"outputs":[],"source":["def plot_attention(attention): \n","    # Function for plotting attention\n","    # You need to get a diagonal plot\n","    plt.clf()\n","    sns.heatmap(attention, cmap='GnBu')\n","    plt.show()\n","\n","class Multi_Attention(torch.nn.Module):\n","    '''\n","    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n","    Here are different ways to compute attention and context:\n","    After obtaining the raw weights, compute and return attention weights and context as follows.:\n","    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n","    attention           = softmax(masked_raw_weights)\n","    context             = bmm(attention, value)\n","    At the end, you can pass context through a linear layer too.\n","    '''\n","    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size, head):\n","        super(Multi_Attention, self).__init__()\n","        self.encoder_hidden_size = encoder_hidden_size \n","        self.key_projection     = nn.Linear(2 * encoder_hidden_size, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n","        self.value_projection   = nn.Linear(2 * encoder_hidden_size, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n","        self.query_projection   = nn.Linear(decoder_output_size, projection_size)# TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n","        # Optional : Define an nn.Linear layer which projects the context vector\n","        self.softmax            = nn.Softmax(dim = 1)# TODO: Define a softmax layer. Think about the dimension which you need to apply \n","        self.head = head\n","        self.projection_size = projection_size\n","        self.attend = Attend()\n","\n","        self.comb = nn.Linear(projection_size, projection_size)\n","        # Tip: What is the shape of energy? And what are those?\n","\n","    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n","    # This function is used to calculate them and set them to self\n","    def set_key_value_mask(self, encoder_outputs, encoder_lens): # B x L x D, B\n","        self.B = encoder_outputs.size(0)\n","        self.L = encoder_outputs.size(1)\n","        _, encoder_max_seq_len, _ = encoder_outputs.shape\n","        self.key      = self.key_projection(encoder_outputs)# TODO: Project encoder_outputs using key_projection to get keys, B x L x decoder_output_size\n","        self.value    = self.value_projection(encoder_outputs)# TODO: Project encoder_outputs using value_projection to get values \n","        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n","        # The raw_weights are of shape (batch_size, timesteps)\n","        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n","        # The mask is False for all indicies before padding begins, True for all indices after.\n","        # 1 x L >= B x 1 --> B x L, Ture is padding, False is not padding \n","        #self.padding_mask     =  torch.arange(encoder_outputs.size(1)).unsqueeze(0) >= encoder_lens.unsqueeze(1) # TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n","        # (Hint: Broadcasting gives you a one liner)\n","        #B x L, Ture is padding, False is not padding \n","        self.padding_mask     = torch.arange(encoder_outputs.size(1)).unsqueeze(0) >= encoder_lens.unsqueeze(1)\n","        \n","    def forward(self, decoder_output_embedding):\n","        # key   : (batch_size, timesteps, projection_size)\n","        # value : (batch_size, timesteps, projection_size)\n","        # query : (batch_size, projection_size)\n","        # print(self.encoder_hidden_size)\n","        self.query         = self.query_projection(decoder_output_embedding)# TODO: Project the query using query_projection\n","\n","        com_query         = torch.reshape(self.query, (self.B, 1 , self.head, self.projection_size // self.head)) # B x proj --> B x 1 x head x proj/head\n","        com_query         = torch.transpose(com_query, 1, 2) # B x 1 x head x proj/head -->  B x head x 1 x proj/head\n","        com_key =  torch.reshape(self.key, (self.B, self.L, self.head, self.projection_size // self.head)) # B x L x proj --> B x L x head x proj/head\n","        com_value = torch.reshape(self.value, (self.B, self.L, self.head, self.projection_size // self.head)) # B x L x proj --> B x L x head x proj/head\n","        com_key = torch.transpose(com_key, 1, 2)#  B x L x head x proj/head  --> B x head x L x proj/head \n","        com_value = torch.transpose(com_value, 1, 2) #  B x L x head x proj/head  --> B x head x L x proj/head\n","        mask = self.padding_mask\n","        for i in range(self.head):\n","          query = com_query[:, i , 0,:]\n","          key = com_key[:, i, :, :]\n","          value = com_value[:,i, :, :]\n","          if i == 0:\n","            context, attention_weights = self.attend(query, key, value, mask) #B x projection_size, B x L\n","            # Bxprojection  -->B x (head x projection)\n","          else:\n","            next_context, next_attention_weights = self.attend(query, key, value, mask) #B x projection_size, B x L\n","            context = torch.cat((context, next_context), 1)\n","            attention_weights = torch.cat((attention_weights, next_attention_weights), 1)\n","        \n","        context = self.comb(context)\n","        #attention_weights = self.comb(attention_weights)\n","\n","\n","        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n","        return context, attention_weights # Return the context, attention_weights, B x projection_size, B x L"]},{"cell_type":"markdown","metadata":{"id":"78XOdWExMSi-"},"source":["## Decoder"]},{"cell_type":"markdown","metadata":{"id":"AVb-uQsAKqqa"},"source":["### Speller"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pnhkqbKMk6jz"},"source":["#### Beam(batch size = 1)(slow)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671158581899,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"ksG44_YhKqqa"},"outputs":[],"source":["def prune(new_beams, beam_width, batch_size):\n","    \"\"\"\n","    beams = [[char, predictions, char_prob, hidden_states, context, log_probs, attention_plot]]\n","    \"\"\"\n","    if len(new_beams) <= beam_width:\n","        return new_beams\n","    beams = new_beams[ :beam_width]\n","    for i in range(batch_size):\n","      new_beams = sorted(new_beams, key = lambda x:x[5][i])\n","      for j in range(beam_width):\n","          for k in range(7):\n","              if k != 1 and k != 3 and k != 6:\n","                  beams[j][k][i] = new_beams[j][k][i]\n","              elif k != 3:\n","                  for h in range(len(beams[j][k])):\n","                    beams[j][k][h][i] = new_beams[j][k][h][i]\n","              else:\n","                  for h in range(2):\n","                      for h1 in range(len(beams[j][k][h])):\n","                          beams[j][k][h][h1][i] = new_beams[j][k][h][h1][i]\n","\n","    return beams"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1671158582167,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"F4GOapelKqqa"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller_beam(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None, beam = 10):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                # Create Two LSTM Cells as per LAS Architecture\n","                                # What should the input_size of the first LSTM Cell? \n","                                # Hint: It takes in a combination of the character embedding and context from attention\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size)\n","                                )\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","        self.beam_width = beam\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        # Set Attention Key, Value, Padding Mask just once\n","        # key   : (batch_size, timesteps, projection_size)\n","        # value : (batch_size, timesteps, projection_size)\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = []\n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        char_prob       = torch.full((batch_size,), fill_value=1.0, dtype= torch.long).to(DEVICE)\n","        #path_end        = torch.full((batch_size,), fill_value=False, dtype= torch.long).to(DEVICE) \n","        log_probs       =  [0.0] * batch_size  \n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        attention_plot          = []\n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        \n","        beams = [[char, predictions, char_prob, hidden_states, context, log_probs, attention_plot]]\n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            new_beams = []\n","            for beam in beams:\n","              char = beam[0]\n","              predictions = beam[1]\n","              char_prob = beam[2]\n","              hidden_states = beam[3]\n","              context = beam[4]\n","              log_probs = beam[5]\n","              #path_end = beam[6]\n","              attention_plot = beam[6]\n","              if self.training:\n","                  # TODO: We want to decide which embedding to use as input for the decoder during training\n","                  # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","                  # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","                  # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","                  if np.random.random() <= tf_rate :\n","                      teacher_forcing = True \n","                  else:\n","                      teacher_forcing = False \n","                  if t == 0:\n","                      char_embed = self.embedding(char)\n","                  else:\n","                    if teacher_forcing:\n","                        # Use ground truth\n","                        char_embed = label_embed[:, t-1, :]\n","                    else:\n","                        if Gumbel:\n","                            char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                        else:\n","                            char_embed = self.embedding(char)\n","              else:\n","                  if Gumbel:\n","                      char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                  else:\n","                      char_embed = self.embedding(char)\n","                  #char_embed = self.embedding(char)\n","              # char_embed = self.embedding(char)#TODO: Generate the embedding for the character at timestep t\n","              # if self.training and t > 0:\n","              #     # TODO: We want to decide which embedding to use as input for the decoder during training\n","              #     # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","              #     # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","              #     # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","              #     char_embed = self.embedding(char)# TODO\n","              # B x embed_size  + B x projection_size ---> B x (embed_size + projection_size)\n","              # print('char_embed: ' + str(char_embed.shape))\n","              # print('context:' + str(context.shape))\n","              decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","              # Loop over your lstm cells\n","              # Each lstm cell takes in an embedding \n","              for i in range(len(self.lstm_cells)):\n","                  # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n","                  # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n","                  # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input,\n","                  # along with the hidden and cell states of the cell from the previous timestep\n","                  # B x (embed_size + projection_size), hidden_states[i]--> h, c to hidden_states[i]\n","                  # decoder_input_embedding: B x decoder_hidden_size\n","                  hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n","                  decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","              # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","              decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","              # We compute attention from the output of the last LSTM Cell\n","              # key   : (batch_size, timesteps, projection_size)\n","              # value : (batch_size, timesteps, projection_size)\n","              # print('deconder_output_embedding:' + str(decoder_output_embedding.shape))\n","              if self.attention != None:\n","                  context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","              attention_plot.append(attention_weights[0].detach().cpu())\n","              # B x projection_size + B x projection_size = B x 2 projection_size\n","              # print('attention.query:' + str(self.attention.query.shape))\n","              # print('context:' + str(context.shape))\n","              output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","              # TODO: Concatenate the projected query with context for the output embedding\n","              # Hint: How can you get the projected query from attention\n","              # If you are not using attention, what will you use instead of query?\n","              char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","              # print('char_prob:' + str(char_prob.shape))\n","              # Append the character probability distribution to the list of predictions \n","              predictions.append(char_prob)\n","\n","              # DrawFromDistribution\n","              # for i in range(char_prob.size(1)):\n","              #     char = torch.full((batch_size,), fill_value= i, dtype= torch.long).to(DEVICE) \n","              #     #char = torch.argmax(char_prob, dim = 1)# TODO: Get the predicted character for the next timestep from the probability distribution\n","              #     log_probs += torch.log(char_prob[:, i]).detach().cpu().numpy()\n","              #     # (Hint: Use Greedy Decoding for starters)\n","              #     # print('char: ' + str(char.shape))\n","              #     new_beam = [char, predictions, char_prob, hidden_states, context, log_probs, attention_plot]\n","              #     new_beams.append(new_beam)\n","              values, indices = torch.topk(char_prob, self.beam_width, dim = 1) #B x beam_width\n","              for i in range(self.beam_width):\n","                  char = indices[:, i]\n","                  logprob = log_probs + torch.log(values[:, i]).detach().cpu().numpy()\n","                  # char = torch.argmax(char_prob, dim = 1)\n","                  # log_probs += torch.log(torch.max(char_prob, dim = 1)[0]).detach().cpu().numpy()\n","                  new_beam = [char, predictions, char_prob, hidden_states, context, logprob, attention_plot]\n","                  new_beams.append(new_beam)\n","            beams = prune(new_beams, self.beam_width,batch_size)\n","        beam = beams[0] # choose the top 1 result\n","        predictions = beam[1]       \n","        attention_plot = beam[6]\n","        attention_plot  = torch.stack(attention_plot, dim =1)# TODO: Stack list of attetion_plots \n","        predictions     = torch.stack(predictions, dim = 1)# TODO: Stack list of predictions \n","\n","        return predictions, attention_plot"]},{"cell_type":"markdown","metadata":{"id":"leZCqfzcLbv_"},"source":["#### Beam2（batchsize >1）\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1671158582403,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"L8WVmjl2Le9r"},"outputs":[],"source":["def prune(new_beams, beam_width):\n","    \"\"\"\n","    beam = [char, predictions, char_prob, hidden_states, context, logprob, seq, attention_plot]\n","    char: B, , predicttions:  B x vocab_size x T , char_prob: B x vocab_size\n","    hidden_states: len(self.lstm_cells) x [h_1, c_1], h_1, c_1: batch x hidden_size\n","    context: B x projection_size\n","    logprob: B\n","    seq : BxL\n","    attention_plot: T x attention_weights[0].shape\n","    batch_size = 96\n","    \"\"\"\n","    char = new_beams[0][0].unsqueeze(1)\n","    predictions = new_beams[0][1].unsqueeze(1)\n","    char_prob = new_beams[0][2].unsqueeze(1)\n","    context = new_beams[0][4].unsqueeze(1)\n","    logprob = new_beams[0][5].unsqueeze(1)\n","    seq = new_beams[0][6].unsqueeze(1) \n","    # attention_plot = new_beams[0][7].unsqueeze(1)\n","\n","    h0 = new_beams[0][3][0][0].unsqueeze(1)\n","    c0 = new_beams[0][3][0][1].unsqueeze(1)\n","    h1 = new_beams[0][3][1][0].unsqueeze(1)\n","    c1 = new_beams[0][3][1][1].unsqueeze(1)\n","    h2 = new_beams[0][3][2][0].unsqueeze(1)\n","    c2 = new_beams[0][3][2][1].unsqueeze(1)\n","\n","    n = len(new_beams)\n","    beams = []\n","    for i in range(1, n):\n","        char = torch.cat((char, new_beams[i][0].unsqueeze(1)), dim = 1)\n","        predictions = torch.cat((predictions, new_beams[i][1].unsqueeze(1)), dim = 1)\n","        char_prob = torch.cat((char_prob, new_beams[i][2].unsqueeze(1)), dim = 1)\n","        context = torch.cat((context, new_beams[i][4].unsqueeze(1)), dim = 1)\n","        logprob = torch.cat((logprob, new_beams[i][5].unsqueeze(1)), dim = 1) #n x B\n","        seq = torch.cat((seq, new_beams[i][6].unsqueeze(1)), dim = 1)\n","        # attention_plot = torch.cat((attention_plot, new_beams[i][7].unsqueeze(1)), dim = 1)\n","\n","        h0 = torch.cat((h0, new_beams[i][3][0][0].unsqueeze(1)), dim = 1)\n","        c0 = torch.cat((c0, new_beams[i][3][0][1].unsqueeze(1)), dim = 1)\n","        h1 = torch.cat((h1, new_beams[i][3][1][0].unsqueeze(1)), dim = 1)\n","        c1 = torch.cat((c1, new_beams[i][3][1][1].unsqueeze(1)), dim = 1)\n","        h2 = torch.cat((h2, new_beams[i][3][2][0].unsqueeze(1)), dim = 1)\n","        c2 = torch.cat((c2, new_beams[i][3][2][1].unsqueeze(1)), dim = 1)\n","    value, indice = torch.topk(logprob, beam_width, dim = 1) # B x n -->indice = B x beam_with\n","    for i in range(beam_width):\n","        batch_indice = indice[:, i] # i-th (B,)\n","        one = F.one_hot(batch_indice, num_classes=n) # B x N\n","        i_char = torch.sum(one * char, dim = 1)\n","        i_predictions = torch.sum(one.unsqueeze(2).unsqueeze(3) * predictions, dim = 1)\n","        i_char_prob = torch.sum(one.unsqueeze(2) * char_prob, dim = 1)\n","        i_context = torch.sum(one.unsqueeze(2) * context, dim = 1)\n","        i_logprob = torch.sum(one * logprob, dim = 1)\n","        i_seq = torch.sum(one.unsqueeze(2) * seq, dim = 1)\n","        # i_attention_plot = torch.sum(one.unsqueeze(2) * attention_plot, dim = 1)\n","\n","        i_h0 = torch.sum(one.unsqueeze(2) * h0, dim = 1)\n","        i_c0 = torch.sum(one.unsqueeze(2) * c0, dim = 1)\n","        i_h1 = torch.sum(one.unsqueeze(2) * h1, dim = 1)\n","        i_c1 = torch.sum(one.unsqueeze(2) * c1, dim = 1)\n","        i_h2 = torch.sum(one.unsqueeze(2) * h2, dim = 1)\n","        i_c2 = torch.sum(one.unsqueeze(2) * c2, dim = 1)\n","\n","        i_hidden_states = [(i_h0, i_c0), (i_h1, i_c1), (i_h2, i_c2)]\n","\n","        beam = [i_char, i_predictions.contiguous(), i_char_prob, i_hidden_states, i_context, i_logprob, i_seq]\n","        beams.append(beam)\n","    return beams"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":167,"status":"ok","timestamp":1671158582567,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"X1kDdi1vZWP-"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size,lstmcell_drop, attention_module= None, beam = 3):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size)\n","                                )\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","        self.beam_width = beam\n","        self.drop = nn.Dropout(p = lstmcell_drop)\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = [] \n","        attention_plot  = []\n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE)\n","        seq            = torch.full((batch_size, 1), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        char_prob       = torch.full((batch_size,), fill_value=1.0, dtype= torch.long).to(DEVICE)\n","        #path_end        = torch.full((batch_size,), fill_value=False, dtype= torch.long).to(DEVICE) \n","        log_probs       =  torch.full((batch_size,), fill_value=0.0).to(DEVICE)\n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        \n","        beams = [[char, predictions, char_prob, hidden_states, context, log_probs, seq]]\n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            new_beams = []\n","            if self.training:\n","                # TODO: We want to decide which embedding to use as input for the decoder during training\n","                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","                if np.random.random() <= tf_rate :\n","                    teacher_forcing = True \n","                else:\n","                    teacher_forcing = False \n","            for num, beam in enumerate(beams):\n","                char = beam[0].clone()\n","                if t > 0:\n","                  predictions = beam[1].clone()\n","                char_prob = beam[2].clone()\n","                hidden_states = beam[3].copy()\n","                context = beam[4].clone()\n","                log_probs = beam[5].clone()\n","                seq = beam[6].clone()\n","                # if t > 0:\n","                #   attention_plot = beam[7].clone()\n","\n","                if self.training:\n","                    if t == 0:\n","                        char_embed = self.embedding(char)\n","                    else:\n","                      if teacher_forcing:\n","                          # Use ground truth\n","                          char_embed = label_embed[:, t-1, :]\n","                      else:\n","                          if Gumbel:\n","                              char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                          else:\n","                              char_embed = self.embedding(char)\n","                else:\n","                    if torch.sum(char) == EOS_TOKEN * batch_size: \n","                        new_beams.append(beam)\n","                        continue\n","                    char_embed = self.embedding(char)\n","                decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","                # Loop over your lstm cells\n","                # Each lstm cell takes in an embedding \n","                for i in range(len(self.lstm_cells)):\n","                    hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n","                    h0 = self.drop(hidden_states[i][0])\n","                    c0 = self.drop(hidden_states[i][1])\n","                    hidden_states[i] = (h0, c0)\n","                    decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","                # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","                decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","                # if t > 0:\n","                #     attention_plot = torch.cat((attention_plot, attention_weights[0].unsqueeze(-1)), dim = -1)# 1 x attention_weights[0].shape\n","                # else:\n","                #     attention_plot = attention_weights[0].unsqueeze(-1)\n","                #attention_plot.append(attention_weights[0].detach().cpu())\n","                if self.attention != None:\n","                    context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","                output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","                char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","                prob = F.softmax(char_prob)\n","                if t > 0:\n","                    predictions = torch.cat((predictions, char_prob.unsqueeze(2)), dim = 2)\n","                else:\n","                    predictions = char_prob.unsqueeze(2) # B x vocab_size x T\n","\n","\n","                #predictions.append(char_prob)\n","\n","                values, indices = torch.topk(prob, self.beam_width, dim = 1) # B x beam_width, B x beam_width\n","                for i in range(self.beam_width):\n","                    char = indices[:, i] # B \n","                    logprob = log_probs + torch.log(values[:, i]).to(DEVICE) # B  + B  = B \n","                    #seq = torch.cat((seq, char.unsqueeze(1)), dim = 1)\n","                    new_beam = [char, predictions, char_prob, hidden_states, context, logprob, torch.cat((seq, char.unsqueeze(1)), dim = 1)]\n","                    new_beams.append(new_beam)\n","\n","            beams = prune(new_beams, self.beam_width)\n","\n","        beam = beams[0] # choose the top 1 result\n","        predictions = beam[1]       \n","        predictions     = torch.transpose(predictions, 1, 2).contiguous()#torch.stack(predictions, dim = 1)# TODO: Stack list of predictions \n","        seq = beam[6]  # B x T\n","        # attention_plot = beam[7]\n","        # attention_plot  = torch.transpose(predictions, 1, 2 )# torch.stack(attention_plot, dim =1)\n","        return predictions, seq # B x T"]},{"cell_type":"markdown","metadata":{"id":"S9xtOT2Bk_QV"},"source":["#### Beam node(fail)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671158582567,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"FAWLGzsYlDep"},"outputs":[],"source":["class BeamSearchNode(object):\n","    def __init__(self, prev_node, char, predictions, char_prob, hidden_states, context, log_probs, attention_plot, length):\n","        self.prev_node = prev_node\n","        self.char = char\n","        self.predictions = predictions\n","        self.char_prob = char_prob\n","        self.hidden_states = hidden_states\n","        self.context = context\n","        self.log_probs = log_probs\n","        self.attention_plot =  attention_plot\n","        self.length = length"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671158582567,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"qI72ut-RnFOj"},"outputs":[],"source":["def prune(new_beams, score, beam_width, batch_size):\n","    \"\"\"\n","    node = BeamSearchNode (prev_node, char, predictions, char_prob, hidden_states, context, log_probs, attention_plot)\n","    score\n","    \"\"\"\n","    if len(new_beams) <= beam_width:\n","        return new_beams\n","    a = new_beams\n","    values, indices = torch.topk(torch.tensor(score), beam_width) \n","    beams = []\n","    for i in range(beam_width):\n","        beams.append(new_beams[indices[i]])\n","    return beams"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1671158582726,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"zo01fMaKlFtn"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None, beam = 4):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                # Create Two LSTM Cells as per LAS Architecture\n","                                # What should the input_size of the first LSTM Cell? \n","                                # Hint: It takes in a combination of the character embedding and context from attention\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size)\n","                                )\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","        self.beam_width = beam\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        # Set Attention Key, Value, Padding Mask just once\n","        # key   : (batch_size, timesteps, projection_size)\n","        # value : (batch_size, timesteps, projection_size)\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = None\n","        attention_plot  = None\n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        char_prob       = torch.full((batch_size,), fill_value=1.0, dtype= torch.long).to(DEVICE)\n","        #path_end        = torch.full((batch_size,), fill_value=False, dtype= torch.long).to(DEVICE) \n","        log_probs       =  torch.full((batch_size,), fill_value=0.0).to(DEVICE) \n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        beam = []\n","        for i in range(batch_size):\n","            node = BeamSearchNode(None, char[i], predictions[i], char_prob[i], hidden_states, context, log_probs, attention_plot, 0)\n","            beam.append(node)\n","        beams = [[beam]]\n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            new_beams = []\n","            score = []\n","            for beam in beams:\n","              char = node.char\n","              predictions = node.predictions\n","              char_prob = node.char_prob\n","              hidden_states = node.hidden_states\n","              context = node.context\n","              log_probs = node.log_probs\n","\n","              if self.training:\n","                  # TODO: We want to decide which embedding to use as input for the decoder during training\n","                  # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","                  # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","                  # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","                  if np.random.random() <= tf_rate :\n","                      teacher_forcing = True \n","                  else:\n","                      teacher_forcing = False \n","                  if t == 0:\n","                      char_embed = self.embedding(char)\n","                  else:\n","                    if teacher_forcing:\n","                        # Use ground truth\n","                        char_embed = label_embed[:, t-1, :]\n","                    else:\n","                        if Gumbel:\n","                            char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                        else:\n","                            char_embed = self.embedding(char)\n","              else:\n","                  if Gumbel:\n","                      char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                  else:\n","                      char_embed = self.embedding(char)\n","              decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","              # Loop over your lstm cells\n","              # Each lstm cell takes in an embedding \n","              for i in range(len(self.lstm_cells)):\n","                  hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n","                  decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","              # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","              decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","              if self.attention != None:\n","                  context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","              attention_plot = attention_weights[0].detach().cpu()\n","              # B x projection_size + B x projection_size = B x 2 projection_size\n","              # print('attention.query:' + str(self.attention.query.shape))\n","              # print('context:' + str(context.shape))\n","              output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","              char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","              # print('char_prob:' + str(char_prob.shape))\n","              # Append the character probability distribution to the list of predictions \n","              predictions = char_prob\n","              # DrawFromDistribution\n","              values, indices = torch.topk(char_prob, self.beam_width, dim = 1) #B x beam_width\n","              for i in range(self.beam_width):\n","                  char = indices[:, i]\n","                  logprob = log_probs + torch.log(values[:, i])\n","                  new_node = BeamSearchNode(node, char, predictions, char_prob, hidden_states, context, logprob, attention_plot)\n","                  new_beams.append(new_node)\n","                  score.append(torch.sum(logprob))\n","            beams = prune(new_beams, score, self.beam_width, batch_size)\n","        node = beams[0] # choose the top 1 result\n","        predictions = []\n","        attention_plot = []\n","        for t in range(timesteps):\n","            predictions.append(node.predictions)\n","            attention_plot.append(node.attention_plot)\n","            node = node.prev_node     \n","        predictions     = predictions[::-1]# TODO: Stack list of predictions \n","        attention_plot  = attention_plot[::-1]\n","        attention_plot  = torch.stack(attention_plot, dim =1)# TODO: Stack list of attetion_plots \n","        predictions     = torch.stack(predictions, dim = 1)# TODO: Stack list of predictions\n","        return predictions, attention_plot"]},{"cell_type":"markdown","metadata":{"id":"6lVhvJSAlA99"},"source":["#### Greedy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1671158582835,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"VwUven7VKqqc"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, lstmcell_drop, attention_module= None):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                # Create Two LSTM Cells as per LAS Architecture\n","                                # What should the input_size of the first LSTM Cell? \n","                                # Hint: It takes in a combination of the character embedding and context from attention\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                \n","                                # multi-head two layer\n","                                )\n","        self.drop = nn.Dropout(p = lstmcell_drop)\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        # global a\n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        # Set Attention Key, Value, Padding Mask just once\n","        # key   : (batch_size, timesteps, projection_size)\n","        # value : (batch_size, timesteps, projection_size)\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = []\n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        attention_plot          = []\n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        \n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            if self.training:\n","                # TODO: We want to decide which embedding to use as input for the decoder during training\n","                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","                if np.random.random() <= tf_rate :\n","                    teacher_forcing = True \n","                else:\n","                    teacher_forcing = False \n","                if t == 0:\n","                    char_embed = self.embedding(char)\n","                else:\n","                  if teacher_forcing:\n","                      # Use ground truth\n","                      char_embed = label_embed[:, t-1, :]\n","                  else:\n","                      if Gumbel:\n","                          char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                      else:\n","                          char_embed = self.embedding(char)\n","\n","            else:\n","                if Gumbel:\n","                    char_embed = torch.nn.functional.gumbel_softmax(char_prob).mm(self.embedding.weight)\n","                else:\n","                    char_embed = self.embedding(char)\n","                #char_embed = self.embedding(char)\n","\n","            # char_embed = self.embedding(char)#TODO: Generate the embedding for the character at timestep t\n","            # if self.training and t > 0:\n","            #     # TODO: We want to decide which embedding to use as input for the decoder during training\n","            #     # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n","            #     # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n","            #     # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n","            #     char_embed = self.embedding(char)# TODO\n","            # B x embed_size  + B x projection_size ---> B x (embed_size + projection_size)\n","            # print('char_embed: ' + str(char_embed.shape))\n","            # print('context:' + str(context.shape))\n","            decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","            # Loop over your lstm cells\n","            # Each lstm cell takes in an embedding \n","            for i in range(len(self.lstm_cells)):\n","                # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n","                # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n","                # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input,\n","                # along with the hidden and cell states of the cell from the previous timestep\n","                # B x (embed_size + projection_size), hidden_states[i]--> h, c to hidden_states[i]\n","                # decoder_input_embedding: B x decoder_hidden_size\n","                hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i])\n","                # print(hidden_states[i])\n","                # print(len(hidden_states[i]))\n","                # print(hidden_states[i][0])\n","                # print(hidden_states[i][0].shape)\n","                # a = hidden_states[i]\n","                h0 = self.drop(hidden_states[i][0])\n","                c0 = self.drop(hidden_states[i][1])\n","                hidden_states[i] = (h0, c0)\n","                decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","            # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","            decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","            # We compute attention from the output of the last LSTM Cell\n","            # key   : (batch_size, timesteps, projection_size)\n","            # value : (batch_size, timesteps, projection_size)\n","            # print('deconder_output_embedding:' + str(decoder_output_embedding.shape))\n","            if self.attention != None:\n","                context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","            attention_plot.append(attention_weights[0].detach().cpu())\n","            # B x projection_size + B x projection_size = B x 2 projection_size\n","            # print('attention.query:' + str(self.attention.query.shape))\n","            # print('context:' + str(context.shape))\n","            output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","            # TODO: Concatenate the projected query with context for the output embedding\n","            # Hint: How can you get the projected query from attention\n","            # If you are not using attention, what will you use instead of query?\n","            char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","            # print('char_prob:' + str(char_prob.shape))\n","            # Append the character probability distribution to the list of predictions \n","            predictions.append(char_prob)\n","\n","            # DrawFromDistribution\n","            char = torch.argmax(char_prob, dim = 1)# TODO: Get the predicted character for the next timestep from the probability distribution \n","            # (Hint: Use Greedy Decoding for starters)\n","            # print('char: ' + str(char.shape))\n","\n","        attention_plot  = torch.stack(attention_plot, dim =1)# TODO: Stack list of attetion_plots \n","        predictions     = torch.stack(predictions, dim = 1)# TODO: Stack list of predictions \n","\n","        return predictions, attention_plot"]},{"cell_type":"markdown","metadata":{"id":"lMgncQmVMnCO"},"source":["## Sequence-to-Sequence Model"]},{"cell_type":"markdown","metadata":{"id":"LWWzurvXM0iv"},"source":["### LAS"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671158582835,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"zcTC4cK95TYT"},"outputs":[],"source":["class LAS(torch.nn.Module):\n","    def __init__(self, \n","                 input_size,\n","                 cnn,\n","                 layer,\n","                 cnn_drop,\n","                 lstm_drop,\n","                 lstmcell_drop,\n","                 encoder_hidden_size, \n","                 vocab_size, \n","                 embed_size,\n","                 decoder_hidden_size, \n","                 decoder_output_size,\n","                 projection_size= 128):\n","        \n","        super(LAS, self).__init__()\n","        self.encoder        = Listener(input_size = input_size, \n","                                       cnn = cnn, \n","                                       encoder_hidden_size = encoder_hidden_size,\n","                                       layer = layer,\n","                                       cnn_drop = cnn_drop,\n","                                       lstm_drop= lstm_drop)# TODO: Initialize Encoder\n","        attention_module    = Attention(encoder_hidden_size = encoder_hidden_size, \n","                                        decoder_output_size= decoder_hidden_size, \n","                                        projection_size=projection_size,\n","                                        )# TODO: Initialize Attention\n","        self.decoder        = Speller(embed_size = embed_size,\n","                                      decoder_hidden_size = decoder_hidden_size, \n","                                      decoder_output_size = decoder_output_size, \n","                                      vocab_size = vocab_size,\n","                                      lstmcell_drop = lstmcell_drop,\n","                                      attention_module= attention_module)# TODO: Initialize Decoder, make sure you pass the attention module \n","\n","    def forward(self, x, x_lens, y = None, tf_rate = 1, gumbel = False):\n","        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n","        # print('encoder_Outputs:' + str(encoder_outputs.shape))\n","        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate, Gumbel = gumbel)\n","        \n","        return predictions, attention_plot"]},{"cell_type":"markdown","metadata":{"id":"EHMzR6fLht5n"},"source":["# Training Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671158582836,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"yWY4zymr32NS"},"outputs":[],"source":["# Global config dict. Feel free to add or change if you want.\n","config = {\n","    'batch_size': 96,\n","    'epochs': 60,\n","    'lr': 1e-3,\n","    'model': 'try6_mask',\n","    'cnn' : [64,128],\n","    'layer' : [3, 3],\n","    'cnn_drop' : 0.1,\n","    'lstm_drop': [0.2,0.2,0.2,0.2],\n","    'encoder_hidden_size' : 256,  \n","    'embed_size' : 256,\n","    'decoder_hidden_size' : 256, \n","    'decoder_output_size' : 128,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epo9wruX7hst"},"outputs":[],"source":["# Global config dict. Feel free to add or change if you want.\n","config = {\n","    'batch_size': 96,\n","    'epochs': 60,\n","    'lr': 1e-3,\n","    'model': 'try7',\n","    'cnn' : [64,128],\n","    'layer' : [3, 3],\n","    'cnn_drop' : 0.1,\n","    'lstm_drop': [0.2,0.3,0.4,0.5],\n","    'encoder_hidden_size' : 256,  \n","    'embed_size' : 256,\n","    'decoder_hidden_size' : 512, \n","    'decoder_output_size' : 128,\n","}"]},{"cell_type":"markdown","metadata":{"id":"TI2AKhQ6YP6F"},"source":["## Model Setup\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IS-YUHQlYQmL"},"outputs":[],"source":["# Baseline LAS has the following configuration:\n","# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n","# Decoder Embedding Layer Dimension of 256\n","# Decoder Hidden Dimension of 512 \n","# Decoder Output Dimension of 128\n","# Attention Projection Size of 128\n","# Feel Free to Experiment with this \n","model = LAS(\n","    # Initialize your model \n","    # Read the paper and think about what dimensions should be used\n","    # You can experiment on these as well, but they are not requried for the early submission\n","    # Remember that if you are using weight tying, some sizes need to be the same\n","          input_size = 15, \n","          cnn = [64, 128],\n","          layer = [3, 3],\n","          cnn_drop = 0.1,\n","          lstm_drop = [0.3,0.3,0.3,0.3],\n","          lstmcell_drop = 0.10,\n","          encoder_hidden_size = 256, \n","          vocab_size = len(VOCAB), \n","          embed_size = 256,\n","          decoder_hidden_size =  256, \n","          decoder_output_size = 128,\n","          projection_size= 128\n",")\n","print(model)\n","model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671158583515,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"WdvVDJ8RsWOG"},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"TDmcYul-YSdC"},"source":["## Optimizer, Scheduler, Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671158583516,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"wgZe3wHP51r0"},"outputs":[],"source":["class XEntLoss(torch.nn.Module):\n","    def __init__(self):\n","        super(XEntLoss, self).__init__()\n","        \n","    def forward(self, prob_list, y):\n","       prob_list = F.softmax(prob_list, dim = 2)\n","       y_one = F.one_hot(y, num_classes = len(VOCAB))\n","       loss = torch.sum(- y_one[:, :, :-1] *torch.log(prob_list[:, :, :-1]), dim = 2)\n","       return loss.view(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671158583516,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"_HwmgDSvbtmd"},"outputs":[],"source":["optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr'], amsgrad= True, weight_decay= 5e-6)\n","criterion   = torch.nn.CrossEntropyLoss(reduction='none') # Why are we using reduction = 'none' ? \n","#criterion = nn.CTCLoss(blank = VOCAB_MAP[\" \"])\n","# criterion = XEntLoss()\n","scaler      = torch.cuda.amp.GradScaler()\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.98, patience=4, verbose=True, threshold=1e-2)\n","# Optional: Create a custom class for a Teacher Force Schedule "]},{"cell_type":"markdown","metadata":{"id":"baHCja89YV-m"},"source":["# Levenshtein Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671158583516,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"GDYZnnLbqJ8J"},"outputs":[],"source":["# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n","def indices_to_chars(indices, vocab):\n","    tokens = []\n","    for i in indices: # This loops through all the indices\n","        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n","            continue\n","        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n","            break\n","        else:\n","            tokens.append(vocab[i])\n","    return tokens\n","\n","# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n","def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n","    dist                = 0\n","    batch_size, seq_len = predictions.shape\n","    for batch_idx in range(batch_size): \n","        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n","        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n","        # Strings - When you are using characters from the AudioDataset\n","        y_string    = ''.join(y_sliced)\n","        pred_string = ''.join(pred_sliced)\n","        dist        += Levenshtein.distance(pred_string, y_string)\n","        # Comment the above abd uncomment below for toy dataset \n","        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n","    if print_example: \n","        # Print y_sliced and pred_sliced if you are using the toy dataset\n","        print(\"Ground Truth : \", y_string)\n","        print(\"Prediction   : \", pred_string)\n","        \n","    dist/=batch_size\n","    return dist"]},{"cell_type":"markdown","metadata":{"id":"1zjfF88iZ4Nc"},"source":["# Train and Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671158583516,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"wIXzhQclhs98"},"outputs":[],"source":["from tqdm.std import TRLock\n","def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n","    # model.to(DEVICE)\n","    model.train()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    running_loss        = 0.0\n","    running_perplexity  = 0.0\n","    \n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","\n","        optimizer.zero_grad()\n","\n","        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n","\n","        with torch.cuda.amp.autocast():\n","\n","            predictions, attention_plot = model(x, lx, y, tf_rate= teacher_forcing_rate, gumbel = True)\n","            # print('1')\n","            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n","            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n","            # So in total, you have batch_size*timesteps amount of characters.\n","            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n","            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n","            loss        =  criterion(predictions.view(-1, predictions.size(2)), y.view(-1)) # TODO: Cross Entropy Loss\n","            # print('2')\n","            # loss = criterion(predictions, y)\n","            max_len     = y.shape[1]\n","            # 1 x L >= B x 1 --- > B x L\n","            mask     =  torch.arange(max_len).unsqueeze(0) <= ly.unsqueeze(1)\n","            # TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n","            mask = mask.to(DEVICE)\n","            # print('3')\n","            masked_loss = torch.sum(loss * mask.view(-1)) / torch.sum(mask)\n","            # Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too \n","            perplexity  = torch.exp(masked_loss) # Perplexity is defined the exponential of the loss\n","            # print('4')\n","            running_loss        += masked_loss.item()\n","            running_perplexity  += perplexity.item()\n","        \n","        # Backward on the masked loss\n","        scaler.scale(masked_loss).backward()\n","\n","        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n","        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n","        \n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(running_loss/(i+1)),\n","            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n","            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n","            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    running_loss /= len(dataloader)\n","    running_perplexity /= len(dataloader)\n","    batch_bar.close()\n","\n","    return running_loss, running_perplexity, attention_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671158583517,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"jzpCjd9R5VYV"},"outputs":[],"source":["def validate(model, dataloader):\n","\n","    model.eval()\n","\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","\n","    running_lev_dist = 0.0\n","\n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","\n","        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n","\n","        with torch.inference_mode():\n","            predictions, attentions = model(x, lx, y = None)\n","\n","        # Greedy Decoding\n","        greedy_predictions   =  predictions.argmax(-1).detach().cpu().numpy()# TODO: How do you get the most likely character from each distribution in the batch?\n","\n","        # Calculate Levenshtein Distance\n","        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n","\n","        batch_bar.set_postfix(\n","            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    running_lev_dist /= len(dataloader)\n","\n","    return running_lev_dist#, running_loss, running_perplexity, "]},{"cell_type":"markdown","metadata":{"id":"Hha5C-fcBc29"},"source":["# Train and Evaluate(Beam)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl1Ko9tmBc2-"},"outputs":[],"source":["from tqdm.std import TRLock\n","def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n","    # model.to(DEVICE)\n","    model.train()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    running_loss        = 0.0\n","    running_perplexity  = 0.0\n","    \n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","\n","        optimizer.zero_grad()\n","\n","        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n","\n","        with torch.cuda.amp.autocast():\n","\n","            predictions, attention_plot = model(x, lx, y, tf_rate= teacher_forcing_rate, gumbel = True)\n","            # print('1')\n","            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n","            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n","            # So in total, you have batch_size*timesteps amount of characters.\n","            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n","            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n","            loss        =  criterion(predictions.view(-1, predictions.size(2)), y.view(-1)) # TODO: Cross Entropy Loss\n","            # print('2')\n","            # loss = criterion(predictions, y)\n","            max_len     = y.shape[1]\n","            # 1 x L >= B x 1 --- > B x L\n","            mask     =  torch.arange(max_len).unsqueeze(0) <= ly.unsqueeze(1)\n","            # TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n","            mask = mask.to(DEVICE)\n","            # print('3')\n","            masked_loss = torch.sum(loss * mask.view(-1)) / torch.sum(mask)\n","            # Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too \n","            perplexity  = torch.exp(masked_loss) # Perplexity is defined the exponential of the loss\n","            # print('4')\n","            running_loss        += masked_loss.item()\n","            running_perplexity  += perplexity.item()\n","        \n","        # Backward on the masked loss\n","        scaler.scale(masked_loss).backward()\n","\n","        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n","        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n","        \n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","\n","        batch_bar.set_postfix(\n","            loss=\"{:.04f}\".format(running_loss/(i+1)),\n","            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n","            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n","            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    running_loss /= len(dataloader)\n","    running_perplexity /= len(dataloader)\n","    batch_bar.close()\n","\n","    return running_loss, running_perplexity, attention_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"g0TItGeUBc2-"},"outputs":[],"source":["def validate(model, dataloader):\n","\n","    model.eval()\n","\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","\n","    running_lev_dist = 0.0\n","\n","    for i, (x, y, lx, ly) in enumerate(dataloader):\n","\n","        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n","\n","        with torch.inference_mode():\n","            predictions, seq = model(x, lx, y = None)\n","\n","        # Greedy Decoding\n","        #greedy_predictions   =  predictions.argmax(-1).detach().cpu().numpy()# TODO: How do you get the most likely character from each distribution in the batch?\n","        #print(seq.shape)\n","        greedy_predictions  = seq.cpu().numpy()\n","        # Calculate Levenshtein Distance\n","        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n","\n","        batch_bar.set_postfix(\n","            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n","        batch_bar.update()\n","\n","        del x, y, lx, ly\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    running_lev_dist /= len(dataloader)\n","\n","    return running_lev_dist#, running_loss, running_perplexity, "]},{"cell_type":"markdown","metadata":{"id":"x-eM8zsIETP1"},"source":["# Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1671158583630,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"GFwn7OZhEStS","outputId":"cf8d5c54-a442-4ef8-ac64-4c5e52b04e54"},"outputs":[],"source":["import wandb\n","wandb.login(key=\"\") "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":1433,"status":"ok","timestamp":1671209901572,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"qVVo8bAab_5N","outputId":"f7bb26a0-7faf-4c67-d84b-427598b22da3"},"outputs":[],"source":["# Login to Wandb\n","# Initialize your Wandb Run Here\n","# Optional: Save your model architecture in a txt file, and save the file to Wandb\n","run = wandb.init(\n","    name = config['model'], ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    project = '', entity = '',\n","    config = config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1671158590282,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"AaQ0Tte6HZ28","outputId":"1c35c218-e0ef-4980-eb5e-1d501e187e64"},"outputs":[],"source":["artifact = run.use_artifact('', type='model')\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":355,"status":"ok","timestamp":1671158590634,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"G8mNkX5pHfov"},"outputs":[],"source":["model.load_state_dict(torch.load('')['model_state_dict'])\n","optimizer.load_state_dict(torch.load('')['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9PqaMBGNldrU"},"outputs":[],"source":["model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5uZJKXlrI22"},"outputs":[],"source":["train_dist = validate(model, train_loader1)"]},{"cell_type":"markdown","metadata":{"id":"99rJQUdPkCUq"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":140,"status":"ok","timestamp":1671209881797,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"MJPvshi3KxDU"},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.95, patience=1, verbose=True, threshold=1e-2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1671209880091,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"HxdQaVjzUYML"},"outputs":[],"source":["optimizer.param_groups[0]['lr'] = 0.001\n","tf_rate =1\n","best_lev_dist = 1500"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lxe_E5ImCWa3"},"outputs":[],"source":["# best_lev_dist = 1e6\n","# tf_rate = 1.0\n","#model.to(DEVICE)\n","for epoch in range(0, config['epochs']):\n","    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n","    # Call train and validate \n","    train_loss, train_perplexity, attention_plot = train(model, train_loader, criterion, optimizer, teacher_forcing_rate=tf_rate) \n","    val_dist = validate(model, val_loader)\n","    scheduler.step(val_dist)\n","    # Print your metrics\n","    print(\"\\nEpoch {}/{}: \\t Train Loss {:.04f} \\t Train perplexity {:.04f} \".format(\n","          epoch + 1,\n","          config['epochs'],\n","          train_loss,\n","          train_perplexity,\n","          ))\n","    print(\"Val dist {:.04f}\\t \".format(val_dist))\n","    wandb.log({\"train_loss\":train_loss, \n","               'train_perplexity': train_perplexity,\n","              'validation_dist':val_dist, \n","               'lr' : optimizer.param_groups[0]['lr'],\n","              'teacher_forcing_rate': tf_rate})\n","    # Plot Attention \n","    # plot_attention(attention_plot)\n","    # if epoch > 10 : \n","    #   scheduler.step(val_dist)\n","    #if (epoch + 1) % 5 == 0:\n","    if tf_rate > 0.6 and val_dist < 12:\n","        tf_rate -= 0.03\n","    # if (epoch + 1) % 10 == 0:\n","    #     train_dist = validate(model, train_loader)\n","    #     print(\"Train dist {:.04f}\\t \".format(train_dist))\n","      \n","    # Log metrics to Wandb\n","\n","    # Optional: Scheduler Step / Teacher Force Schedule Step\n","    torch.cuda.empty_cache()\n","\n","    if val_dist <= best_lev_dist:\n","        best_lev_dist = val_dist\n","        # Save your model checkpoint here\n","        print(\"Saving model\")\n","        # Saving the model and optimizer states\n","        torch.save({\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              #'scheduler_state_dict':scheduler.state_dict()\n","              'val_dist': val_dist, \n","               'epoch': epoch\n","              }, \"Model\")\n","        # Creating Artifact\n","        model_artifact = wandb.Artifact(config['model'], type='model')\n","        # Adding model file to Artifact\n","        model_artifact.add_file(\"Model\")\n","        # Saving Artifact to WandB\n","        run.log_artifact(model_artifact)\n","        best_val_dist = val_dist\n","\n","run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcjxwWQVNBDR"},"outputs":[],"source":["torch.save({\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              #'scheduler_state_dict':scheduler.state_dict()\n","              'val_dist': val_dist, \n","               'epoch': epoch\n","              }, \"Model\")\n","# Creating Artifact\n","model_artifact = wandb.Artifact(config['model'], type='model')\n","# Adding model file to Artifact\n","model_artifact.add_file(\"Model\")\n","# Saving Artifact to WandB\n","run.log_artifact(model_artifact)"]},{"cell_type":"markdown","metadata":{"id":"qG_k6eKtZtor"},"source":["# Testing"]},{"cell_type":"markdown","metadata":{"id":"7HN2IUmV2HBf"},"source":["## Greedy search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50QMMnxjZtor"},"outputs":[],"source":["# Optional: Load your best model Checkpoint here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S419ipE0cZt0"},"outputs":[],"source":["class LAS(torch.nn.Module):\n","    def __init__(self, \n","                 input_size,\n","                 cnn,\n","                 layer,\n","                 cnn_drop,\n","                 lstm_drop,\n","                 encoder_hidden_size, \n","                 vocab_size, \n","                 embed_size,\n","                 decoder_hidden_size, \n","                 decoder_output_size,\n","                 projection_size= 128):\n","        \n","        super(LAS, self).__init__()\n","        self.encoder        = Listener(input_size = input_size, \n","                                       cnn = cnn, \n","                                       encoder_hidden_size = encoder_hidden_size,\n","                                       layer = layer,\n","                                       cnn_drop = cnn_drop,\n","                                       lstm_drop= lstm_drop)# TODO: Initialize Encoder\n","        attention_module    = Attention(encoder_hidden_size = encoder_hidden_size, \n","                                        decoder_output_size= decoder_hidden_size, \n","                                        projection_size=projection_size,\n","                                        )# TODO: Initialize Attention\n","        self.decoder        = Speller(embed_size = embed_size,\n","                                      decoder_hidden_size = decoder_hidden_size, \n","                                      decoder_output_size = decoder_output_size, \n","                                      vocab_size = vocab_size, \n","                                      attention_module= attention_module)# TODO: Initialize Decoder, make sure you pass the attention module \n","\n","    def forward(self, x, x_lens, y = None, tf_rate = 1, gumbel = False):\n","        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n","        # print('encoder_Outputs:' + str(encoder_outputs.shape))\n","        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate, Gumbel = gumbel)\n","        \n","        return predictions, attention_plot"]},{"cell_type":"markdown","metadata":{"id":"EMfzdyU12Ocf"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z32STN50cdv3"},"outputs":[],"source":["# Baseline LAS has the following configuration:\n","# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n","# Decoder Embedding Layer Dimension of 256\n","# Decoder Hidden Dimension of 512 \n","# Decoder Output Dimension of 128\n","# Attention Projection Size of 128\n","# Feel Free to Experiment with this \n","model = LAS(\n","    # Initialize your model \n","    # Read the paper and think about what dimensions should be used\n","    # You can experiment on these as well, but they are not requried for the early submission\n","    # Remember that if you are using weight tying, some sizes need to be the same\n","          input_size = 15, \n","          cnn = [64, 128],\n","          layer = [3, 3],\n","          cnn_drop = 0.1,\n","          lstm_drop = [0.2,0.3,0.3,0.2],\n","          encoder_hidden_size = 256, \n","          vocab_size = len(VOCAB), \n","          embed_size = 256,\n","          decoder_hidden_size =  256, \n","          decoder_output_size = 128,\n","          projection_size= 128\n",")\n","print(model)\n","model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCvBMiIaZtos"},"outputs":[],"source":["artifact = run.use_artifact('', type='model')\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GPDi5NJZtos"},"outputs":[],"source":["model.load_state_dict(torch.load('')['model_state_dict'])\n","optimizer.load_state_dict(torch.load('')['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1r7txgpzZtos"},"outputs":[],"source":["# TODO: Create a testing function similar to validation \n","def test(model, dataloader):\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","    pred = []\n","    for i, (x, lx) in enumerate(dataloader):\n","        x, lx = x.to(DEVICE), lx\n","        with torch.inference_mode():\n","            predictions, attentions = model(x, lx, y = None)\n","        # Greedy Decoding\n","        greedy_predictions   =  predictions.argmax(-1).detach().cpu().numpy()# TODO: How do you get the most likely character from each distribution in the batch?\n","        print(greedy_predictions[0].shape)\n","        batch_size= predictions.shape[0]\n","        for batch_idx in range(batch_size): \n","          pred_sliced = indices_to_chars(greedy_predictions[batch_idx], VOCAB)\n","          # Strings - When you are using characters from the AudioDataset\n","          pred_string = ''.join(pred_sliced)\n","          pred.append(pred_string)\n","        del x, lx\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jac_c15VZtos"},"outputs":[],"source":["torch.cuda.empty_cache()\n","predictions = test(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0XM6GjFZtos"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame()\n","df['id'] = [i for i in range(len(predictions))]\n","df['label'] = predictions\n","df.to_csv('submission.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4NBzJp4Ztos"},"outputs":[],"source":["# TODO: Submit to Kaggle\n","!kaggle competitions submit -c <competition> -f 'submission.csv' -m \"I made it!\""]},{"cell_type":"markdown","metadata":{"id":"jgu1VmUxbcji"},"source":["# Testing(Beam2)"]},{"cell_type":"markdown","metadata":{"id":"dfMxdVbUbcjm"},"source":["#### Beam(batch size >1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJ874cdtbcjm"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller_beam(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None, beam = 10):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size)\n","                                )\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","        self.beam_width = beam\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = [] \n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE)\n","        seq            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        char_prob       = torch.full((batch_size,), fill_value=1.0, dtype= torch.long).to(DEVICE)\n","        #path_end        = torch.full((batch_size,), fill_value=False, dtype= torch.long).to(DEVICE) \n","        log_probs       =  [0.0] * batch_size  \n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        \n","        beams = [[char, predictions, char_prob, hidden_states, context, log_probs, seq]]\n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            new_beams = []\n","            for num, beam in enumerate(beams):\n","                char = beam[0].clone()\n","                if char == EOS_TOKEN :\n","                    new_beams.append(beam.copy())\n","                    continue\n","                predictions = beam[1].copy()\n","                # print('time: '+str(t) + ', num: ' + str(num) + ', len: ' + str(len(predictions)))\n","                char_prob = beam[2].clone()\n","                hidden_states = beam[3].copy()\n","                context = beam[4].clone()\n","                log_probs = beam[5].copy()\n","                seq = beam[6].clone()\n","\n","                char_embed = self.embedding(char)\n","                \n","                decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","                # Loop over your lstm cells\n","                # Each lstm cell takes in an embedding \n","                for i in range(len(self.lstm_cells)):\n","                    hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n","                    decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","                # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","                decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","                \n","                if self.attention != None:\n","                    context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","                output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","                char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","                prob = F.softmax(char_prob)\n","                predictions.append(char_prob)\n","\n","                values, indices = torch.topk(prob, self.beam_width, dim = 1) #B x beam_width\n","                for i in range(self.beam_width):\n","                    char = indices[:, i]\n","                    logprob = log_probs + torch.log(values[:, i]).detach().cpu().numpy()\n","                    # char = torch.argmax(char_prob, dim = 1)\n","                    # log_probs += torch.log(torch.max(char_prob, dim = 1)[0]).detach().cpu().numpy()\n","                    #print('values: ' + str(values[:, i]) + ', log: ' + str(logprob))\n","                    new_beam = [char, predictions, char_prob, hidden_states, context, logprob, torch.cat((seq, char))]\n","                    new_beams.append(new_beam)\n","\n","            beams = prune(new_beams, self.beam_width, batch_size)\n","\n","        beam = beams[0] # choose the top 1 result\n","        predictions = beam[1]       \n","        predictions     = torch.stack(predictions, dim = 1)# TODO: Stack list of predictions \n","        seq = beam[-1]\n","        return predictions, seq"]},{"cell_type":"markdown","metadata":{"id":"qwWdl0NJn-s4"},"source":["#### Beam (batch size = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3o2IJa4d6j3"},"outputs":[],"source":["def prune(new_beams, beam_width, batch_size):\n","    \"\"\"\n","    beams = [[char, predictions, char_prob, hidden_states, context, log_probs, attention_plot]]\n","    batch_size = 1\n","    \"\"\"\n","    new_beams = sorted(new_beams, key = lambda x:x[5], reverse=True)\n","    if len(new_beams) <= beam_width:\n","        return new_beams\n","    beams = new_beams[ :beam_width]\n","    return beams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8b-Vzg0n-s5"},"outputs":[],"source":["from torch._C import get_num_interop_threads\n","class Speller(torch.nn.Module):\n","\n","    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None, beam = 10):\n","        super().__init__()\n","        self.vocab_size         = vocab_size\n","        self.embedding          = nn.Embedding(num_embeddings = self.vocab_size, \n","                                               embedding_dim = embed_size,\n","                                               padding_idx = padding)\n","        # TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n","        self.lstm_cells         = torch.nn.Sequential(\n","                                nn.LSTMCell(input_size = embed_size + decoder_output_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size),\n","                                nn.LSTMCell(input_size = decoder_hidden_size, \n","                                            hidden_size = decoder_hidden_size)\n","                                )\n","                                # We are using LSTMCells because process individual time steps inputs and not the whole sequence.\n","                                # Think why we need this in terms of the query\n","        self.char_prob          = nn.Linear(2 * decoder_output_size, vocab_size)\n","        # TODO: Initialize the classification layer to generate your probability distribution over all characters\n","        self.char_prob.weight   = self.embedding.weight # Weight tying\n","        self.attention          = attention_module\n","\n","        self.beam_width = beam\n","\n","    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1, Gumbel = False): \n","        '''\n","        Args: \n","            embedding: Attention embeddings \n","            hidden_list: List of Hidden States for the LSTM Cells\n","        ''' \n","        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape # B, L\n","        if self.training:\n","            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n","            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n","            # B x seq_size --> B x seq_size x embed_size\n","        else:\n","            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n","        if self.attention != None:\n","            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n","        # INITS\n","        predictions     = [] \n","        # Initialize the first character input to your decoder, SOS, O(-1) = SOS, (B x 1)\n","        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE)\n","        seq            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n","        char_prob       = torch.full((batch_size,), fill_value=1.0, dtype= torch.long).to(DEVICE)\n","        #path_end        = torch.full((batch_size,), fill_value=False, dtype= torch.long).to(DEVICE) \n","        log_probs       =  [0.0] * batch_size  \n","        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n","        hidden_states   = [None]*len(self.lstm_cells) #len(self.decoder.lstm_cells) \n","        context                 = self.attention.value[:, 0, :]# TODO: Initialize context (You have a few choices, refer to the writeup ) C(-1) = V(0)\n","        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n","        \n","        beams = [[char, predictions, char_prob, hidden_states, context, log_probs, seq]]\n","        for t in range(timesteps):\n","            #TODO: Generate the embedding for the character at timestep t\n","            # B x embed_size\n","            new_beams = []\n","            for num, beam in enumerate(beams):\n","                char = beam[0].clone()\n","                if char == EOS_TOKEN :\n","                    new_beams.append(beam.copy())\n","                    continue\n","                predictions = beam[1].copy()\n","                # print('time: '+str(t) + ', num: ' + str(num) + ', len: ' + str(len(predictions)))\n","                char_prob = beam[2].clone()\n","                hidden_states = beam[3].copy()\n","                context = beam[4].clone()\n","                log_probs = beam[5].copy()\n","                seq = beam[6].clone()\n","\n","                char_embed = self.embedding(char)\n","                \n","                decoder_input_embedding =  torch.cat((char_embed, context), dim = 1) # TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n","                # Loop over your lstm cells\n","                # Each lstm cell takes in an embedding \n","                for i in range(len(self.lstm_cells)):\n","                    hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n","                    decoder_input_embedding = hidden_states[i][0] # B x decoder_hidden_size\n","                # The output embedding from the decoder is the hidden state of the last LSTM Cell\n","                decoder_output_embedding = hidden_states[-1][0] # B x decoder_hidden_size\n","                \n","                if self.attention != None:\n","                    context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n","                output_embedding     =  torch.cat((self.attention.query, context), dim=1)\n","                char_prob            = self.char_prob(output_embedding) # B x vocab_size\n","                prob = F.softmax(char_prob)\n","                predictions.append(char_prob)\n","\n","                values, indices = torch.topk(prob, self.beam_width, dim = 1) #B x beam_width\n","                for i in range(self.beam_width):\n","                    char = indices[:, i]\n","                    logprob = log_probs + torch.log(values[:, i]).detach().cpu().numpy()\n","                    # char = torch.argmax(char_prob, dim = 1)\n","                    # log_probs += torch.log(torch.max(char_prob, dim = 1)[0]).detach().cpu().numpy()\n","                    #print('values: ' + str(values[:, i]) + ', log: ' + str(logprob))\n","                    new_beam = [char, predictions, char_prob, hidden_states, context, logprob, torch.cat((seq, char))]\n","                    new_beams.append(new_beam)\n","\n","            beams = prune(new_beams, self.beam_width, batch_size)\n","\n","        beam = beams[0] # choose the top 1 result\n","        predictions = beam[1]       \n","        predictions     = torch.stack(predictions, dim = 1)# TODO: Stack list of predictions \n","        seq = beam[-1]\n","        return predictions, seq"]},{"cell_type":"markdown","metadata":{"id":"MLwffd0fbcjm"},"source":["## Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySDrANcioJG5"},"outputs":[],"source":["class LAS(torch.nn.Module):\n","    def __init__(self, \n","                 input_size,\n","                 cnn,\n","                 layer,\n","                 cnn_drop,\n","                 lstm_drop,\n","                 encoder_hidden_size, \n","                 vocab_size, \n","                 embed_size,\n","                 decoder_hidden_size, \n","                 decoder_output_size,\n","                 projection_size= 128,\n","                 beam = 10):\n","        \n","        super(LAS, self).__init__()\n","        self.encoder        = Listener(input_size = input_size, \n","                                       cnn = cnn, \n","                                       encoder_hidden_size = encoder_hidden_size,\n","                                       layer = layer,\n","                                       cnn_drop = cnn_drop,\n","                                       lstm_drop= lstm_drop)# TODO: Initialize Encoder\n","        attention_module    = Attention(encoder_hidden_size = encoder_hidden_size, \n","                                        decoder_output_size= decoder_hidden_size, \n","                                        projection_size=projection_size\n","                                        )# TODO: Initialize Attention\n","        self.decoder        = Speller_beam(embed_size = embed_size,\n","                                      decoder_hidden_size = decoder_hidden_size, \n","                                      decoder_output_size = decoder_output_size, \n","                                      vocab_size = vocab_size, \n","                                      attention_module= attention_module,\n","                                      beam = beam)# TODO: Initialize Decoder, make sure you pass the attention module \n","\n","    def forward(self, x, x_lens, y = None, tf_rate = 1, gumbel = False):\n","        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n","        # print('encoder_Outputs:' + str(encoder_outputs.shape))\n","        predictions, seq = self.decoder(encoder_outputs, encoder_lens, y, tf_rate, Gumbel = gumbel)\n","        \n","        return predictions, seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOU2v5zbbcjm"},"outputs":[],"source":["# Baseline LAS has the following configuration:\n","# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n","# Decoder Embedding Layer Dimension of 256\n","# Decoder Hidden Dimension of 512 \n","# Decoder Output Dimension of 128\n","# Attention Projection Size of 128\n","# Feel Free to Experiment with this \n","model = LAS(\n","    # Initialize your model \n","    # Read the paper and think about what dimensions should be used\n","    # You can experiment on these as well, but they are not requried for the early submission\n","    # Remember that if you are using weight tying, some sizes need to be the same\n","          input_size = 15, \n","          cnn = [64, 128],\n","          layer = [3, 3],\n","          cnn_drop = 0.1,\n","          lstm_drop = [0.2,0.3,0.3,0.2],\n","          encoder_hidden_size = 256, \n","          vocab_size = len(VOCAB), \n","          embed_size = 256,\n","          decoder_hidden_size =  256, \n","          decoder_output_size = 128,\n","          projection_size= 128\n",")\n","print(model)\n","model.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXUQbeeybcjm"},"outputs":[],"source":["artifact = run.use_artifact('', type='model')\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKVE99Mzbcjm"},"outputs":[],"source":["model.load_state_dict(torch.load('')['model_state_dict'])\n","optimizer.load_state_dict(torch.load('')['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ljLRaaIbcjn"},"outputs":[],"source":["# TODO: Create a testing function similar to validation \n","def test(model, dataloader):\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","    pred = []\n","    batch_size = config['batch_size']\n","    for i, (x, lx) in enumerate(dataloader):\n","        x, lx = x.to(DEVICE), lx\n","        with torch.inference_mode():\n","            predictions, seq = model(x, lx, y = None)\n","        # Greedy Decoding\n","        #greedy_predictions   =  predictions.argmax(-1).detach().cpu().numpy()# TODO: How do you get the most likely character from each distribution in the batch?\n","        #print(seq.shape)\n","        greedy_predictions  = seq.cpu().numpy()# TODO: How do you get the most likely character from each distribution in the batch?\n","        batch_size= greedy_predictions.shape[0]\n","        for batch_idx in range(batch_size): \n","          pred_sliced = indices_to_chars(greedy_predictions[batch_idx], VOCAB)\n","          # Strings - When you are using characters from the AudioDataset\n","          pred_string = ''.join(pred_sliced)\n","          pred.append(pred_string)\n","        del x, lx\n","        torch.cuda.empty_cache()\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    return pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JoIwLdS6l2h"},"outputs":[],"source":["torch.cuda.empty_cache()\n","predictions = test(model, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSVaDI6ibcjn"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame()\n","df['id'] = [i for i in range(len(predictions))]\n","df['label'] = predictions\n","df.to_csv('submission.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2378,"status":"ok","timestamp":1670426208864,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"inQVQ14-bcjn","outputId":"14d24e07-d33c-42ec-cc6f-123b7d743180"},"outputs":[],"source":["# TODO: Submit to Kaggle\n","!kaggle competitions submit -c 11-785-f22-hw4p2 -f submission.csv -m \"hahahahaha!\"\n","# !kaggle competitions submit -c <competition> -f 'submission.csv' -m \"I made it!\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OALQCI0EDCwh","X7J4sY1OW9Pr","gQenneVsDLnX","lCbwz0LZMWwe","GuLso2WWOlcA","XoI0zEoIMX5I","JJCpBcEmMVcZ","f6k9R7jKMRcZ","pnhkqbKMk6jz","leZCqfzcLbv_","S9xtOT2Bk_QV","6lVhvJSAlA99","TDmcYul-YSdC","baHCja89YV-m","1zjfF88iZ4Nc","Hha5C-fcBc29","qwWdl0NJn-s4"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
