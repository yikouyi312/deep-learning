{"cells":[{"cell_type":"markdown","metadata":{"id":"F9ERgBpbcMmB"},"source":["# HW1: Frame-Level Speech Recognition"]},{"cell_type":"markdown","metadata":{"id":"CLkH6GMGcWcE"},"source":["In this homework, you will be working with MFCC data consisting of 15 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."]},{"cell_type":"markdown","metadata":{"id":"z4vZbDmJvMp1"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdXId6rCil3V"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwYu9sSUnSho"},"outputs":[],"source":["!pip install torchsummaryX wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI4qfx7tiBZt"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummaryX import summary\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.metrics import accuracy_score\n","import gc\n","import zipfile\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import os\n","import datetime\n","import wandb\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yBgXjKV1O0Z"},"outputs":[],"source":["## If you are using colab, you can import google drive to save model checkpoints in a folder\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-9qE20hmCgQ"},"outputs":[],"source":["### PHONEME LIST\n","PHONEMES = [\n","            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']"]},{"cell_type":"markdown","metadata":{"id":"ZIi0Big7vPa9"},"source":["# Kaggle"]},{"cell_type":"markdown","metadata":{"id":"BBCbeRhixGM7"},"source":["This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3BsEjr7J2bb"},"outputs":[],"source":["#Install Kaggle API\n","!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"\",\"key\":\"\"}') \n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12454,"status":"ok","timestamp":1664206674231,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":240},"id":"6T1dk_dvlQ8i","outputId":"826707f8-9d60-4a66-d9c9-c72b25466a21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 11-785-f22-hw1p2.zip to /content\n","100% 2.12G/2.13G [00:11<00:00, 241MB/s]\n","100% 2.13G/2.13G [00:11<00:00, 204MB/s]\n"]}],"source":["#Download data and unzip\n","! kaggle competitions download -c 11-785-f22-hw1p2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5KrO3a9bWD0"},"outputs":[],"source":["#unzip \n","! unzip -qo '11-785-f22-hw1p2.zip' -d '/content'"]},{"cell_type":"markdown","metadata":{"id":"Vuzce0_TdcaR"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"2_7QgMbBdgPp"},"source":["This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n","\n","Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrqexh-8J1CF"},"outputs":[],"source":["# Dataset class to load train and validation data\n","class AudioDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_path, context, offset=0, partition= \"train-clean-100\", limit=-1): # Feel free to add more arguments\n","\n","        self.context = context #hyperparameter, generally optimal betweeen 0 and 50\n","        self.offset = offset\n","        self.data_path = data_path\n","        # Mel Frequency Cepstral Coefficient (MFCC)\n","      \n","        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' \n","        self.transcript_dir = self.data_path +'/'+ partition + '/transcript' \n","      \n","        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n","        transcript_names = sorted(os.listdir(self.transcript_dir))\n","\n","        assert len(mfcc_names) == len(transcript_names) \n","        self.mfccs, self.transcripts = [], []\n","\n","        for i in range(0, len(mfcc_names)):\n","            mfcc = np.load(self.mfcc_dir + '/' + mfcc_names[i])\n","        #   Gaussin Normalization of mfcc\n","            mean = np.mean(mfcc,axis = 0)\n","            sigma = np.std(mfcc, axis = 0)\n","            mfcc = (mfcc - mean)/sigma\n","\n","        #   Load the corresponding transcript\n","        #   Remove [SOS] and [EOS] from the transcript \n","            transcript = np.load(self.transcript_dir + '/' + transcript_names[i])[1:-1] \n","        #   Append each mfcc to self.mfcc, transcript to self.transcript\n","            self.mfccs.append(mfcc)\n","            self.transcripts.append(transcript)\n","        # NOTE:\n","        # Each mfcc is of shape T1 x 15, T2 x 15, ...\n","        # Each transcript is of shape (T1+2) x 15, (T2+2) x 15 before removing [SOS] and [EOS]\n","        # Concatenate all mfccs in self.X such that the final shape is T x 15 (Where T = T1 + T2 + ...) \n","        self.mfccs = np.concatenate(self.mfccs, axis = 0)\n","        # Concatenate all transcripts in self.Y such that the final shape is (T,) meaning, each time step has one phoneme output\n","        self.transcripts = np.concatenate(self.transcripts)\n","        # Take some time to think about what we have done. self.mfcc is an array of the format (Frames x Features). \n","        # Our goal is to recognize phonemes of each frame\n","        # From hw0, you will be knowing what context is.\n","        # TODOL We can introduce context by padding zeros on top and bottom of self.mfcc\n","        self.mfccs = np.pad(self.mfccs, [(self.context, self.context), (0,0)], 'constant', constant_values=(0, 0)) \n","        # These are the available phonemes in the transcript\n","        self.phonemes = [\n","            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n","        # But the neural network cannot predict strings as such. Instead we map these phonemes to integers\n","        # Map the phonemes to their corresponding list indexes in self.phonemes\n","        self.transcripts = [self.phonemes.index(self.transcripts[i]) for i in range(len(self.transcripts))]\n","        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n","        # Length of the dataset is now the length of concatenated mfccs/transcripts\n","        self.length = len(self.transcripts)\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n","        frames = self.mfccs[ind + self.offset - self.context: ind + self.offset + self.context + 1]\n","        # After slicing, you get an array of shape 2*context+1 x 15. But our MLP needs 1d data and not 2d.\n","        # TODO: Flatten to get 1d data\n","        frames = frames.flatten()\n","        frames = torch.FloatTensor(frames) # Convert to tensors\n","        phoneme = torch.tensor(self.transcripts[ind])       \n","\n","        return frames, phoneme\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8KfVP39S6o7"},"outputs":[],"source":["class AudioTestDataset(torch.utils.data.Dataset):\n","    # Create a test dataset class similar to tclass but you dont have transcripts for this\n","    def __init__(self, data_path, context, offset=0, partition= \"test-clean\", limit=-1): \n","        self.context = context #hyperparameter, generally optimal betweeen 0 and 50\n","        self.offset = offset\n","        self.data_path = data_path\n","        # Mel Frequency Cepstral Coefficient (MFCC)\n","        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' \n","        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n","        self.mfccs = []\n","\n","        for i in range(0, len(mfcc_names)):\n","        #   Load a single mfcc\n","            mfcc = np.load(self.mfcc_dir + '/' + mfcc_names[i])\n","        #   Gaussian Normalization of mfcc\n","            mean = np.mean(mfcc,axis = 0)\n","            sigma = np.std(mfcc, axis = 0)\n","            mfcc = (mfcc - mean)/sigma\n","        #   Append each mfcc to self.mfcc, \n","            self.mfccs.append(mfcc)\n","\n","        self.mfccs = np.concatenate(self.mfccs, axis = 0)\n","        self.mfccs = np.pad(self.mfccs, [(self.context, self.context), (0,0)], 'constant', constant_values=(0, 0)) \n","        self.phonemes = [\n","            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n","            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n","            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n","            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n","            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n","            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n","\n","        self.length = len(self.mfccs) - 2*self.context\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, ind):\n","        frames = self.mfccs[ind + self.offset - self.context: ind + self.offset + self.context + 1]\n","        frames = frames.flatten()\n","        frames = torch.FloatTensor(frames) # Convert to tensors\n","\n","        return frames"]},{"cell_type":"markdown","metadata":{"id":"2mlwaKlDt_2c"},"source":["# Create Train Dataset and Validation Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZQ3pF6R7zlu"},"outputs":[],"source":["def bulid_dataclass(context):\n","  # Create a dataset object using the AudioDataset class for the training data \n","  train_data = AudioDataset('/content', context, offset=context, partition= \"train-clean-100\", limit=-1)\n","  # Create a dataset object using the AudioDataset class for the validation data \n","  val_data = AudioDataset('/content', context, offset=context, partition= \"dev-clean\", limit=-1) \n","  # Create a dataset object using the AudioTestDataset class for the test data \n","  return train_data, val_data"]},{"cell_type":"markdown","metadata":{"id":"irCcf-rjGwde"},"source":["\n","Get subset data for tunning hyperparameter (faster)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qgcj9MIN_CUk"},"outputs":[],"source":["def get_subset(train_data, val_data):\n","  #Get subset of train_data and val_data\n","  torch.manual_seed(0)\n","  # subsetsize\n","  train_indices = torch.randperm(len(train_data))[:len(train_data)]\n","  val_indices = torch.randperm(len(val_data))[:len(val_data)]\n","  train_data = torch.utils.data.Subset(train_data, train_indices)\n","  val_data = torch.utils.data.Subset(val_data, val_indices)\n","  return train_data, val_data"]},{"cell_type":"markdown","metadata":{"id":"q5yZf4zW8ceW"},"source":["Build dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SIRazO66haV"},"outputs":[],"source":["def build_data(batch_size, train_data, val_data):\n","    train_loader = torch.utils.data.DataLoader(train_data, \n","                                               num_workers= 4, \n","                                               batch_size=batch_size, \n","                                               pin_memory= True,shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n","                                            batch_size=batch_size, pin_memory= True,\n","                                            shuffle= False)\n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{"id":"yMd_XxPku5qp"},"source":["# Wandb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCDYx5VEu6qI"},"outputs":[],"source":["wandb.login(key=\"\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"markdown","metadata":{"id":"15a2CypOY0kF"},"source":["# Helper function\n","    evaluation function:\n","        eval(model, dataloader)\n","    train function for 1 epoch:\n","        train(model, optimizer, criterion, dataloader, scaler, scheduler)\n","    train function:\n","        train1(model, train_loader, val_loader, optimizer, criterion, scheduler, scaler)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C98MXtiVozbw"},"outputs":[],"source":["def eval(model, dataloader):\n","    model.eval() # set model in evaluation mode\n","    phone_true_list = []\n","    phone_pred_list = []\n","    for i, data in enumerate(dataloader):\n","        frames, phonemes = data\n","        ### Move data to device (ideally GPU)\n","        frames, phonemes = frames.to(device), phonemes.to(device) \n","        # makes sure that there are no gradients computed as we are not training the model now\n","        # no_grad mode\n","        with torch.inference_mode(): \n","            ### Forward Propagation\n","            logits = model(frames)\n","        ### Get Predictions\n","        predicted_phonemes = torch.argmax(logits, dim=1)\n","        ### Store Pred and True Labels\n","        phone_pred_list.extend(predicted_phonemes.tolist())\n","        phone_true_list.extend(phonemes.tolist())\n","        # Do you think we need loss.backward() and optimizer.step() here?\n","        del frames, phonemes, logits\n","        torch.cuda.empty_cache()\n","    ### Calculate Accuracy\n","    accuracy = accuracy_score(phone_pred_list, phone_true_list) \n","    return accuracy*100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFmFzxazc0tK"},"outputs":[],"source":["def train(model, optimizer, criterion, dataloader, scaler, scheduler):\n","    model.train()\n","    train_loss = 0.0 # Monitoring Loss\n","    #Use coloured version of progress bars\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","    for iter, (mfccs, phonemes) in enumerate(dataloader):\n","        ### Move Data to Device (Ideally GPU)\n","        mfccs = mfccs.to(device)\n","        phonemes = phonemes.to(device)\n","        # Processing inputs and calling backward\n","        with torch.autocast(device):\n","            ### Forward Propagation\n","            logits = model(mfccs)\n","            ### Loss Calculation\n","            loss = criterion(logits, phonemes)\n","            \n","        train_loss += loss.item()\n","        batch_bar.set_postfix(\n","              # acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * run_config['batch_size'])),\n","              loss=\"{:.04f}\".format(float(train_loss / (iter + 1))),\n","              # num_correct=num_correct,\n","              lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","        ### Initialize Gradients\n","        # Zero the gradient buffers of all parameters and backprops with random gradients:\n","        optimizer.zero_grad()\n","       ### Backward Propagation\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        batch_bar.update()\n","        scheduler.step()\n","\n","    batch_bar.close()\n","    train_loss /= len(dataloader)\n","\n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae_2AoD0Y3qj"},"outputs":[],"source":["def train1(model, train_loader, val_loader, optimizer, criterion, scheduler, scaler):\n","  torch.cuda.empty_cache()\n","  best_acc = 0\n","  for epoch in range(config['epochs']):\n","      train_loss = train(model, optimizer, criterion, train_loader, scaler, scheduler)\n","      #train_acc = eval(model, train_loader)\n","      val_acc = eval(model, val_loader)\n","      lr = float(optimizer.param_groups[0]['lr'])\n","      #Train Accuracy {:.04f}%,\n","      print(\"Epoch {}/{}:  Validation Accuracy {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n","          epoch + 1,\n","          config['epochs'],\n","          #train_acc ,\n","          val_acc,\n","          train_loss,\n","          lr\n","          )\n","      )\n","      #schedule lr\n","      #scheduler.step(val_acc)\n","      # What to log \n","      metrics = {\n","          \"train_loss\":train_loss,\n","          #\"train_acc\": train_acc,\n","          'val_acc': val_acc,\n","          'lr': lr\n","      }\n","      # Log to run\n","      wandb.log(metrics)\n","      # Updating the model version\n","      if val_acc > best_acc:\n","        best_acc = val_acc\n","        # Saving the model and optimizer states\n","        torch.save({\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict()\n","              }, \"Model\")\n","        \n","        # Creating Artifact\n","        model_artifact = wandb.Artifact(config['model'], type='model')\n","        # Adding model file to Artifact\n","        model_artifact.add_file(\"Model\")\n","        # Saving Artifact to WandB\n","        run.log_artifact(model_artifact)\n","  \n","  #wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"Nxjwve20JRJ2"},"source":["# Network Architecture\n"]},{"cell_type":"markdown","metadata":{"id":"3NJzT-mRw6iy"},"source":["Define the MLP structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_ynumazqSYE"},"outputs":[],"source":["class Net1(nn.Module):\n","    def __init__(self, context, size1,size2,size3,size4,size5,size6,drop,initial,initial_choice):\n","\n","        super(Net1, self).__init__()\n","\n","        input_size = (2*context + 1) * 15 \n","        layer1 = size1\n","        layer2 = size2\n","        layer3 = size3\n","        layer4 = size4\n","        layer5 = size5\n","        layer6 = size6\n","        output_size = 40 \n","        \n","        self.model = nn.Sequential(\n","              nn.Linear(input_size, layer1, bias=False),\n","              nn.BatchNorm1d(layer1),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer1, layer2, bias=False),\n","              nn.BatchNorm1d(layer2),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer2, layer3, bias=False),\n","              nn.BatchNorm1d(layer3),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer3, layer4, bias=False),\n","              nn.BatchNorm1d(layer4),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer4, layer5, bias=False),\n","              nn.BatchNorm1d(layer5),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer5, layer6, bias=False),\n","              nn.BatchNorm1d(layer6),\n","              nn.ReLU(),\n","              nn.Dropout(drop),\n","\n","              nn.Linear(layer6, output_size),\n","              )\n","        # Initialize the parameters\n","        if initial:\n","          for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","              if initial_choice == 'xavier_uniform':\n","                torch.nn.init.xavier_uniform_(m.weight)\n","              elif initial_choice == 'uniform':\n","                torch.nn.init.uniform_(m.weight)\n","              elif initial_choice == 'normal':\n","                torch.nn.init.normal_(m.weight)\n","              elif initial_choice == 'kaiming_uniform':\n","                torch.nn.init.kaiming_uniform_(m.weight)\n","              elif initial_choice == 'kaiming_normal':\n","                torch.nn.init.kaiming_normal_(m.weight) \n","    # define the forward function\n","    # the backward function (where gradients are computed)\n","    # is automatically defined using autograd\n","    def forward(self, x):\n","        out = self.model(x)\n","    # .parameters() return learnable parameters of a model\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"eYDulW60Y-TY"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1664206849943,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":240},"id":"XblOHEVtKab2","outputId":"fc51a863-f6e9-4018-f5ad-d3e49af96cfd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["425"]},"metadata":{},"execution_count":31}],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"UWM42D1GYkFJ"},"source":["Parameter Setting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmKwlFqgt_Zq"},"outputs":[],"source":["config = {\n","    'epochs': 40,\n","    'batch_size' : 1024,\n","    'context' : 32,\n","    'learning_rate' : 0.001,\n","    'lr_schedule': 'CosineAnnealing',#'CosineAnnealing', #'StepLR', 'ReduceLROnPlateau', 'Exponential', 'CosineAnnealing'\n","    'optimizer': 'Adamw',#'Nesterov',#'Adam', #'SGD', 'RMSProp'\n","    'weight_decay': 0.02,\n","    'model': \"try4\",\n","    'drop':0.2,\n","    'initial':'kaiming_uniform'\n","}"]},{"cell_type":"markdown","metadata":{"id":"tenVITkvYope"},"source":["Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnnYwWKXOcxL"},"outputs":[],"source":["train_data, val_data = bulid_dataclass(config['context'])\n","#train_data, val_data = get_subset(train_data, val_data)\n","train_loader, val_loader = build_data(config['batch_size'], train_data, val_data)\n","print(\"Batch size: \", config['batch_size'])\n","print(\"Context: \", config['context'])\n","print(\"Input size: \", (2*config['context']+1)*15)\n","print(\"Output symbols: \", len(PHONEMES))\n","print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n","batch_number = len(train_loader) # for CosineAnnealing period"]},{"cell_type":"markdown","metadata":{"id":"Que8XLrTYQeI"},"source":["Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDTJcSGqZJcz"},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = config['model'], ### Wandb creates random run names if you skip this field, we recommend you give useful names\n","    reinit= True, ### Allows reinitalizing runs when you re-run this cell\n","    project =\"hw1\", ### Project should be created in your wandb account \n","    config=config,### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"HD3cycZSZ2_3"},"source":["Initial Model\n","\n","    In this setting, model have 19.864616M parameter\n","    It has 7 layers, inputsize-->2048-->2048-->2048-->2048-->2048-->512-->output\n","    Dropout = 0.2\n","    Weight Initial: kaiming_uniform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2p-tqF5WZj2a"},"outputs":[],"source":["model = Net1(config['context'], 2048,2048,2048,2048,2048,512,config['drop'], True, config['initial']).to(device)\n","# Check number of parameters of your network \n","# - Remember, you are limited to 20 million parameters for HW1 (including ensembles)\n","summary(model, frames.to(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1664206875609,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":240},"id":"wft15E_IxYFi","outputId":"826b2780-156f-4534-f244-ff54e3ac8226"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/wandb/run-20220926_154053-nbnvwa3q/files/model_arch.txt']"]},"metadata":{},"execution_count":35}],"source":["### Save your model architecture as a string with str(model) \n","model_arch = str(model)\n","\n","### Save it in a txt file \n","arch_file = open(\"model_arch.txt\", \"w\")\n","file_write = arch_file.write(model_arch)\n","file_write = arch_file.write(\"\\n\")\n","file_write = arch_file.write(\"parameter setting:\\n\")\n","for key, value in config.items(): \n","        arch_file.write('%s:%s\\n' % (key, value))\n","arch_file.close()\n","\n","### log it in your wandb run with wandb.save()\n","wandb.save('model_arch.txt')"]},{"cell_type":"markdown","metadata":{"id":"aJOmzfwEppgq"},"source":["Define loss function, optimizer and learning rate scheduler. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_v77PfTtSBSU"},"outputs":[],"source":["# Defining Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n","# Define scheduler with inial lr = 0.001\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, batch_number,\n","                                                          eta_min=1e-4, last_epoch=- 1, verbose=False)\n","# Defining Loss function \n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"roCY5SCxaCwC"},"source":["## Train\n","\n","    Due to the limit usage on colab, it cannot train model for epochs bigger than 40. \n","    Reload the previous model and train next 40 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTF51L4VqhuS"},"outputs":[],"source":["train1(model, train_loader, val_loader, optimizer, criterion,scheduler,scaler)"]},{"cell_type":"markdown","metadata":{"id":"ZBwd1TlErMqd"},"source":["Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_5bdIFqrPyL"},"outputs":[],"source":["torch.save({'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()\n","              }, \"Model\")\n","        \n","# Creating Artifact\n","model_artifact = wandb.Artifact(config['model'], type='model')\n","# Adding model file to Artifact\n","model_artifact.add_file(\"Model\")\n","# Saving Artifact to WandB\n","run.log_artifact(model_artifact)\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"VP85metQ4vQw"},"source":["# Retrain\n","\n","    Load previous model and change hyper-parameter to retrain\n","    \n"]},{"cell_type":"markdown","source":["## First Re-Train\n","    1. change CosineAnnealing minimal learing rate between 1e-3 and 1e-5\n","    2. change CosineAnnealing minimal learing rate between 1e-4 and 1e-5"],"metadata":{"id":"cXzfuMXJhckc"}},{"cell_type":"markdown","metadata":{"id":"DOkQzl9OrgTP"},"source":["Define new hyperparameter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsDcu9S_x753"},"outputs":[],"source":["config = {\n","    'epochs': 40,\n","    'batch_size' : 1024,\n","    'context' : 32,\n","    'learning_rate' : 0.001,\n","    'lr_schedule': 'CosineAnnealing',\n","    'optimizer': 'Adamw',\n","    'weight_decay': 0.01,\n","    'model': \"try4\",\n","    'drop':0.2,\n","    'initial':'kaiming_uniform'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHT4qMBrwHrU"},"outputs":[],"source":["import wandb\n","run =  wandb.init(\n","    name = config['model'], ### Wandb creates random run names if you skip this field, we recommend you give useful names\n","    reinit= True, ### Allows reinitalizing runs when you re-run this cell\n","    project =\"hw1\", ### Project should be created in your wandb account \n","    config=config,### Wandb Config for your run\n","    entity=\"\"# add file name\n",")\n","artifact = run.use_artifact('', type='model') # add name\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWqzeFZF6S8y"},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","source":["Load model "],"metadata":{"id":"rN5mz7buhD6V"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pkZSq7cvg3V"},"outputs":[],"source":["# Define model\n","model1 = Net1(config['context'], 2048,2048,2048,2048,2048,512, config['drop'], True, config['initial']).to(device)\n","# Load model state\n","model1.load_state_dict(torch.load('')['model_state_dict']) # add name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzsI3BI5rvA3"},"outputs":[],"source":["# Defining Optimizer\n","optimizer = torch.optim.AdamW(model1.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n","# Load optimizer state\n","optimizer.load_state_dict(torch.load('')['optimizer_state_dict']) # add name"]},{"cell_type":"markdown","source":["Change CosineAnnealingLR minimal learing-rate to 1e-5"],"metadata":{"id":"XMaEvX3rhKW0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGGjbZLGuJAB"},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = batch_number,\n","                                                           eta_min=1e-5, last_epoch=- 1, verbose=False)\n","# Defining Loss function \n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUmdvsJWx08E"},"outputs":[],"source":["train1(model1, train_loader, val_loader, optimizer, criterion, scheduler, scaler)"]},{"cell_type":"markdown","metadata":{"id":"WGEioYDtsRDv"},"source":["Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vpMpwb4OWfd"},"outputs":[],"source":["torch.save({'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()\n","              }, \"Model\")\n","        \n","# Creating Artifact\n","model_artifact = wandb.Artifact(config['model'], type='model')\n","# Adding model file to Artifact\n","model_artifact.add_file(\"Model\")\n","# Saving Artifact to WandB\n","run.log_artifact(model_artifact)\n","wandb.finish()"]},{"cell_type":"markdown","source":["## Second Re-Train\n","    \n","    change dropout from 0.2 to 0.15"],"metadata":{"id":"ek-kc53ah9qo"}},{"cell_type":"markdown","metadata":{"id":"3NN4g7lTh9qo"},"source":["Define new hyperparameter\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cF4zNeQrh9qo"},"outputs":[],"source":["config = {\n","    'epochs': 40,\n","    'batch_size' : 1024,\n","    'context' : 32,\n","    'learning_rate' : 0.0001,\n","    'lr_schedule': 'CosineAnnealing',\n","    'optimizer': 'Adamw',\n","    'weight_decay': 0.01,\n","    'model': \"try4\",\n","    'drop':0.15,\n","    'initial':'kaiming_uniform'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTXALqaHh9qo"},"outputs":[],"source":["import wandb\n","run =  wandb.init(\n","    name = config['model'], ### Wandb creates random run names if you skip this field, we recommend you give useful names\n","    reinit= True, ### Allows reinitalizing runs when you re-run this cell\n","    project =\"hw1\", ### Project should be created in your wandb account \n","    config=config,### Wandb Config for your run\n","    entity=\"11785chong\"\n",")\n","artifact = run.use_artifact('11785chong/hw1/try4:v0', type='model')\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKQNT4Wqh9qp"},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","source":["Load model "],"metadata":{"id":"r5KCobeqh9qp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1x1VnTpXh9qp"},"outputs":[],"source":["# Define model\n","model1 = Net1(config['context'], 2048,2048,2048,2048,2048,512, config['drop'], True, config['initial']).to(device)\n","# Load model state\n","model1.load_state_dict(torch.load('/content/artifacts/try4:v0/Model')['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1g1DGEhh9qp"},"outputs":[],"source":["# Defining Optimizer\n","optimizer = torch.optim.AdamW(model1.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n","# Load optimizer state\n","optimizer.load_state_dict(torch.load('/content/artifacts/try4:v0/Model')['optimizer_state_dict'])"]},{"cell_type":"markdown","source":["Change CosineAnnealingLR min learing-rate to 1e-5"],"metadata":{"id":"_hD0zxaoh9qp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CajgFP-Vh9qp"},"outputs":[],"source":["scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = batch_number,\n","                                                           eta_min=1e-5, last_epoch=- 1, verbose=False)\n","# Defining Loss function \n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTX2Gfthh9qp"},"outputs":[],"source":["train1(model1, train_loader, val_loader, optimizer, criterion, scheduler, scaler)"]},{"cell_type":"markdown","metadata":{"id":"G4P1q2ESh9qp"},"source":["Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcfyPeV9h9qp"},"outputs":[],"source":["torch.save({'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict()\n","              }, \"Model\")\n","        \n","# Creating Artifact\n","model_artifact = wandb.Artifact(config['model'], type='model')\n","# Adding model file to Artifact\n","model_artifact.add_file(\"Model\")\n","# Saving Artifact to WandB\n","run.log_artifact(model_artifact)\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"_kXwf5YUo_4A"},"source":["# Testing and submission to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBrs5SuGXVQQ"},"outputs":[],"source":["test_data = AudioTestDataset('/content', config['context'], offset=config['context'], partition= \"test-clean\", limit=-1) \n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=config['batch_size'], shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-SU9fZ3xHtk"},"outputs":[],"source":["def test(model, test_loader):\n","  ### What you call for model to perform inference?\n","  model.eval()\n","\n","  ### List to store predicted phonemes of test data\n","  test_predictions = []\n","\n","  ### Which mode do you need to avoid gradients?\n","  with torch.inference_mode(): \n","      for i, frames in enumerate(tqdm(test_loader)):\n","\n","          frames = frames.float().to(device)             \n","          \n","          output = model(frames)\n","\n","          ### Get most likely predicted phoneme with argmax\n","          predicted_phonemes = torch.argmax(output, dim=1)\n","\n","          ### How do you store predicted_phonemes with test_predictions? Hint, look at eval \n","          test_predictions.extend(predicted_phonemes.tolist())\n","          \n","  return test_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["bbdb91551c6d402ba15dc4e8acdba457","e27032cc87d54480a5a5fa0a7e7db6dd","67d22a192f5c4dfe9c3cbe2fd0f01d76","0b9d6aeae2284769a63c8eb9dcad2ce5","d35d5d25df5b48028f35531794538ee3","0eb396bc13b342bf9d9ed4c3c7975655","41f84e24db7a4cb7a67ccbc4883f589f","6d45b780ac1145f7a5b920a60f0735ed","1bb43cfc71d0431b9267c4f8b2d8ed9f","c42e1160cb964b049b45752dc4653eaf","c07b80f0097e46c0aceabf3276db252b"]},"executionInfo":{"elapsed":34197,"status":"ok","timestamp":1664041030246,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":240},"id":"wG9v6Xmxu7wp","outputId":"fee5033a-6a78-4174-9ae0-affa364fda41"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1898 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbdb91551c6d402ba15dc4e8acdba457"}},"metadata":{}}],"source":["predictions = test(model1, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZE1hRnvf0bFz"},"outputs":[],"source":["### Create CSV file with predictions\n","with open(\"./submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(predictions)):\n","        f.write(\"{},{}\\n\".format(i, predictions[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3976,"status":"ok","timestamp":1664041035991,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":240},"id":"LjcammuCxMKN","outputId":"6d540b72-28ba-4eb8-980c-93be05bcb2d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n","100% 18.6M/18.6M [00:01<00:00, 10.1MB/s]\n","Successfully submitted to Frame-Level Speech Recognition"]}],"source":["## Submit to kaggle competition using kaggle API\n","!kaggle competitions submit -c 11-785-f22-hw1p2 -f ./submission.csv -m \"Test Submission\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["z4vZbDmJvMp1","ZIi0Big7vPa9","_kXwf5YUo_4A"],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bbdb91551c6d402ba15dc4e8acdba457":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e27032cc87d54480a5a5fa0a7e7db6dd","IPY_MODEL_67d22a192f5c4dfe9c3cbe2fd0f01d76","IPY_MODEL_0b9d6aeae2284769a63c8eb9dcad2ce5"],"layout":"IPY_MODEL_d35d5d25df5b48028f35531794538ee3"}},"e27032cc87d54480a5a5fa0a7e7db6dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb396bc13b342bf9d9ed4c3c7975655","placeholder":"​","style":"IPY_MODEL_41f84e24db7a4cb7a67ccbc4883f589f","value":"100%"}},"67d22a192f5c4dfe9c3cbe2fd0f01d76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d45b780ac1145f7a5b920a60f0735ed","max":1898,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bb43cfc71d0431b9267c4f8b2d8ed9f","value":1898}},"0b9d6aeae2284769a63c8eb9dcad2ce5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c42e1160cb964b049b45752dc4653eaf","placeholder":"​","style":"IPY_MODEL_c07b80f0097e46c0aceabf3276db252b","value":" 1898/1898 [00:34&lt;00:00, 61.09it/s]"}},"d35d5d25df5b48028f35531794538ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb396bc13b342bf9d9ed4c3c7975655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41f84e24db7a4cb7a67ccbc4883f589f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d45b780ac1145f7a5b920a60f0735ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bb43cfc71d0431b9267c4f8b2d8ed9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c42e1160cb964b049b45752dc4653eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c07b80f0097e46c0aceabf3276db252b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}