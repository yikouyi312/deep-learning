{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary"
      ],
      "metadata": {
        "id": "sOxD6-vQI7qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install package"
      ],
      "metadata": {
        "id": "HkqBQE1nJCoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "db4oGSjFJCC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "!pip install torchsummaryX"
      ],
      "metadata": {
        "id": "u_8sPnUCJBJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Package"
      ],
      "metadata": {
        "id": "0VorXVvDKX9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67sD31uxKZ0B",
        "outputId": "e1bee017-3708-4fc3-f688-14ca7ea7f816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google drive"
      ],
      "metadata": {
        "id": "fCM_gOQcLHgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "pyN_Lm5BLKVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c370f681-c108-4b3b-deff-965c73e2312c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle setup"
      ],
      "metadata": {
        "id": "HFbioAY5KrR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"\",\"key\":\"\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IooJPMIKu5w",
        "outputId": "79b35763-df65-4df8-d0e8-7f7de6cc936b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c 11-785-f22-hw3p2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS3OOCDeKyOQ",
        "outputId": "24b4f601-13d8-4555-ab9e-258c61521c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-f22-hw3p2.zip to /content\n",
            "100% 8.87G/8.88G [00:54<00:00, 111MB/s] \n",
            "100% 8.88G/8.88G [00:54<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "!unzip -q 11-785-f22-hw3p2.zip\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L93q0i6QK22W",
        "outputId": "c25e019f-feee-49fc-85e9-0d6907bc2be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "XtklFiRZL2VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "PHONEMES = CMUdict\n",
        "mapping = CMUdict_ARPAbet\n",
        "LABELS = ARPAbet"
      ],
      "metadata": {
        "id": "_5USJhfKL4ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(LABELS))"
      ],
      "metadata": {
        "id": "HPnHR_7ZfFDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84288874-aaed-45e6-a4ad-1dff55c49d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainData"
      ],
      "metadata": {
        "id": "ibp1K3APMI3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "    #TODO\n",
        "    def __init__(self, data_path, partition= \"train-clean-100\", limit=-1): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        ''' \n",
        "        # Load the directory and all files in them\n",
        "        self.data_path = data_path\n",
        "\n",
        "        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' #TODO\n",
        "        self.transcript_dir = self.data_path +'/'+ partition + '/transcript/raw' #TODO\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir)) #TODO\n",
        "\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files) \n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        CMUdict_ARPAbet = {\n",
        "              \"\" : \" \",\n",
        "              \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "              \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "              \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "              \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "              \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "              \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "              \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "              \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "              \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "        CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "        ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "        self.PHONEMES = CMUdict\n",
        "        self.mapping = CMUdict_ARPAbet\n",
        "        self.LABELS = ARPAbet\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in range(0, len(self.mfcc_files)):\n",
        "            mfcc = np.load(self.mfcc_dir + '/' + self.mfcc_files[i])\n",
        "        #   Gaussin Normalization of mfcc\n",
        "            mean = np.mean(mfcc,axis = 0)\n",
        "            sigma = np.std(mfcc, axis = 0)\n",
        "            mfcc = (mfcc - mean)/sigma\n",
        "        #   Load the corresponding transcript\n",
        "        #   Remove [SOS] and [EOS] from the transcript \n",
        "            transcript = np.load(self.transcript_dir + '/' + self.transcript_files[i])\n",
        "            transcript = transcript[1: -1]\n",
        "            # Map to label\n",
        "            transcript = [self.LABELS.index(self.mapping[transcript[index]]) for index in range(len(transcript))]\n",
        "            # Transform to numerical\n",
        "            #transcript = np.array([transcript.index(self.LABELS[i]) for i in range(len(transcript))])\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "            # self.label.append(label)\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = torch.Tensor(self.mfccs[ind])# TODO\n",
        "        transcript = torch.Tensor(self.transcripts[ind])# TODO\n",
        "        #abel = self.labels[ind]\n",
        "        return mfcc, transcript\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        batch_mfcc, batch_transcript = zip(*batch)# TODO\n",
        "        #batch_transcript = [transcript for mfcc, transcript in batch] # TODO\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0) # TODO\n",
        "        lengths_mfcc =  torch.LongTensor([len(x) for x in batch_mfcc])# TODO \n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=0) # TODO\n",
        "        lengths_transcript =  torch.LongTensor([len(x) for x in batch_transcript])# TODO\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "7EM2pendMLH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TestData"
      ],
      "metadata": {
        "id": "0NqUvNNbMUPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "    #TODO\n",
        "    def __init__(self, data_path, partition= \"test-clean\", limit=-1): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        ''' \n",
        "        # Load the directory and all files in them\n",
        "        self.data_path = data_path\n",
        "        self.mfcc_dir = self.data_path +'/'+ partition + '/mfcc' #TODO\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir)) #TODO\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        CMUdict_ARPAbet = {\n",
        "          \"\" : \" \",\n",
        "          \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "          \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "          \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "          \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "          \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "          \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "          \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "          \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "          \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "        CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "        ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "        self.PHONEMES = CMUdict\n",
        "        self.mapping = CMUdict_ARPAbet\n",
        "        self.LABELS = ARPAbet\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs = []\n",
        "        for i in range(0, len(self.mfcc_files)):\n",
        "            mfcc = np.load(self.mfcc_dir + '/' + self.mfcc_files[i])\n",
        "        #   Gaussin Normalization of mfcc\n",
        "            mean = np.mean(mfcc,axis = 0)\n",
        "            sigma = np.std(mfcc, axis = 0)\n",
        "            mfcc = (mfcc - mean)/sigma\n",
        "        #   Append each mfcc to self.mfcc\n",
        "            self.mfccs.append(mfcc)\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "        # raise NotImplemented\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = torch.Tensor(self.mfccs[ind])# TODO\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        batch_mfcc = batch # TODO\n",
        "        #batch_transcript = [transcript for mfcc, transcript in batch] # TODO\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=0.0) # TODO\n",
        "        lengths_mfcc =  torch.LongTensor([len(x) for x in batch_mfcc]) # TODO \n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ],
      "metadata": {
        "id": "bBELXLzhMVsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Hyperparameter"
      ],
      "metadata": {
        "id": "1PoDlt7xMX9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "config = {'batch_size': 64}\n",
        "root = '/content/hw3p2' "
      ],
      "metadata": {
        "id": "yFF2hqWFMcHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "b-zQFCg4Mf8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR_s9KC4MhUJ",
        "outputId": "e7329be4-ea89-41d9-d7b0-6a7dfab8b66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(root, partition= \"train-clean-360\", limit=-1) #TODO\n",
        "val_data = AudioDataset(root, partition= \"dev-clean\", limit=-1) # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "test_data = AudioDatasetTest(root, partition= \"test-clean\", limit=-1) #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, \n",
        "                                           collate_fn=AudioDataset.collate_fn,\n",
        "                                           num_workers= 4, \n",
        "                                           batch_size=config['batch_size'], \n",
        "                                           pin_memory= True,shuffle=True\n",
        "                                           )\n",
        "val_loader = torch.utils.data.DataLoader(val_data, \n",
        "                                         collate_fn=AudioDataset.collate_fn,\n",
        "                                         num_workers= 4, \n",
        "                                         batch_size=config['batch_size'], \n",
        "                                         pin_memory= True,shuffle=False\n",
        "                                         )#TODO\n",
        "test_loader = torch.utils.data.DataLoader(test_data, \n",
        "                                          collate_fn=AudioDatasetTest.collate_fn,\n",
        "                                          num_workers= 4, \n",
        "                                          batch_size=config['batch_size'], \n",
        "                                          pin_memory= True,shuffle=False\n",
        "                                          )#TODO"
      ],
      "metadata": {
        "id": "MYQjm4egMjvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check"
      ],
      "metadata": {
        "id": "reV3BugKlEcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwx8HIVxlC4q",
        "outputId": "519ef94d-4fc0-4e51-edcc-155ccfb0fd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 104014, batches = 1626\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx, ly)\n",
        "    break "
      ],
      "metadata": {
        "id": "u3TEob0OeRvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Config"
      ],
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_SIZE = len(LABELS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ-qQ_Sf-LIu",
        "outputId": "ff3d0aa6-1280-498e-b022-00e857f57c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic"
      ],
      "metadata": {
        "id": "HLad4pChcuvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    \"\"\"Gaussian noise regularizer.\n",
        "\n",
        "    Args:\n",
        "        sigma (float, optional): relative standard deviation used to generate the\n",
        "            noise. Relative means that it will be multiplied by the magnitude of\n",
        "            the value your are adding the noise to. This means that sigma can be\n",
        "            the same regardless of the scale of the vector.\n",
        "        is_relative_detach (bool, optional): whether to detach the variable before\n",
        "            computing the scale of the noise. If `False` then the scale of the noise\n",
        "            won't be seen as a constant but something to optimize: this will bias the\n",
        "            network to generate vectors with smaller values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=True):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x "
      ],
      "metadata": {
        "id": "g4J-_sPak_Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels = 15, out_channels = 64, stride = 1, dropout = 0.1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.ConstantPad1d(3 // 2, 0),\n",
        "                        nn.Conv1d(in_channels, out_channels, kernel_size = 3, stride = stride, bias = False),\n",
        "                        nn.BatchNorm1d(out_channels),\n",
        "                        nn.ReLU(),\n",
        "                        torch.nn.Dropout(dropout),\n",
        "\n",
        "                        nn.ConstantPad1d(3 // 2, 0),\n",
        "                        nn.Conv1d(out_channels, out_channels, kernel_size = 3, stride = 1, bias = False),\n",
        "                        nn.BatchNorm1d(out_channels),\n",
        "                        torch.nn.Dropout(dropout)\n",
        "                        )\n",
        "\n",
        "        self.downsample = downsample\n",
        "        # self.downsample = nn.Sequential(\n",
        "        #         nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
        "        #         nn.BatchNorm1d(64),\n",
        "        #     )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TQdh4Ngn8kw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Class(nn.Module):\n",
        "    def __init__(self, insize = 512, num_classes = 43, dropout = 0.1):\n",
        "        super(Class, self).__init__()\n",
        "        self.linear1 = nn.Linear(insize, 1024, bias = False)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.linear2 = nn.Linear(1024, 2048, bias = False)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(2048)\n",
        "\n",
        "        self.linear3 = nn.Linear(2048, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p = dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.linear3(x)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "UED1bOljXuBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import dropout\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, block, Class, num_classes = 43, embed_drop = 0.1, cls_dropout = 0.1,lstm_drop = 0.1, layer = 1):\n",
        "        super(Network, self).__init__()\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        #self.embedding = torch.nn.Embedding(num_embeddings = , embedding_dim = 15, padding_idx = 0)\n",
        "        self.embedding = self.make_layer(block, 15, 64, block_num = layer, stride = 1, drop = embed_drop)\n",
        "        # self.embedding1 = self.make_layer(block, 64, 128, block_num = layer, stride = 1, drop = embed_drop)\n",
        "        # self.embedding = block(15, 64, stride = 1, dropout = embed_drop)\n",
        "        # self.cnn1 = nn.Conv1d(in_channels = 15, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        # self.batchnorm = nn.BatchNorm1d(64)\n",
        "        # # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        self.lstm = nn.LSTM(input_size = 64, hidden_size = 256, num_layers = 4, batch_first = True, dropout = lstm_drop, bidirectional = True) \n",
        "        self.cls = Class(insize = 512, num_classes = num_classes, dropout = cls_dropout)\n",
        "        self.logSoftmax = torch.nn.LogSoftmax(2)#TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "    def make_layer(self, block, in_channel, out_channel, block_num, stride=1, drop=0.3):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channel != out_channel:\n",
        "            downsample = nn.Sequential(\n",
        "                 nn.Conv1d(in_channel, out_channel, kernel_size=1, stride=1, bias = False),\n",
        "                 nn.BatchNorm1d(out_channel),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(in_channel, out_channel, stride, drop, downsample))\n",
        "        in_channel = out_channel\n",
        "        for i in range(1, block_num):\n",
        "            layers.append(block(in_channel, out_channel, stride, drop))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        #TODO\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        # Refer to the handout for hints\n",
        "        x = self.embedding(x.transpose(1,2))\n",
        "        x = x.transpose(1,2)\n",
        "        # x = self.embedding1(x.transpose(1,2))\n",
        "        # x = x.transpose(1,2)\n",
        "        packed_x = pack_padded_sequence(x, lx, batch_first = True,  enforce_sorted=False)\n",
        "        packed_out = self.lstm(packed_x)[0]\n",
        "        out, out_lens = pad_packed_sequence(packed_out, batch_first=True)\n",
        "        # # Log softmax after output layer is required since`nn.CTCLoss` expects log probabilities.\n",
        "        out = self.cls(out)\n",
        "        out = self.logSoftmax(out)\n",
        "        return out, out_lens"
      ],
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "efVXnarNHiBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "KZqg77vs2RoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"beam_width\" : 2,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 50,\n",
        "    'embed_drop' : 0.6,\n",
        "     'cls_dropout' : 0.6,\n",
        "     'lstm_dropout' : 0.6,\n",
        "     'cnn_layer' : 4,\n",
        "     'weight_decay' : 5e-5,\n",
        "     'model': 'lstm1141'\n",
        "    } # Feel free to add more items here"
      ],
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INIT"
      ],
      "metadata": {
        "id": "tUThsowyQdN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network(block = ResidualBlock, Class = Class, num_classes = 43, embed_drop = config['embed_drop'], cls_dropout = config['cls_dropout'], lstm_drop = config['lstm_dropout'], layer = config['cnn_layer']).to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ],
      "metadata": {
        "id": "FRwSJen7Z8aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Config"
      ],
      "metadata": {
        "id": "IBwunYpyugFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "criterion = nn.CTCLoss(blank = 0)# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay']) # What goes in here?\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(\n",
        "    LABELS,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=40,\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=config['beam_width'],\n",
        "    num_processes=4,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")#TODO \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 2, factor = 0.8)#TODO\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"\")\n",
        "run = wandb.init(\n",
        "    name = config['model'], ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #project=\"Shuxian\", entity=\"hw2_p2\",\n",
        "    project = 'hw3', entity = '',\n",
        "    config = config)"
      ],
      "metadata": {
        "id": "5DD3JXgNj3U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artifact = run.use_artifact('', type='model')\n",
        "artifact_dir = artifact.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sABV79vafRWP",
        "outputId": "cdfaa9ae-d357-43c9-85fd-02b00d36be9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact lstm1141:v43, 93.90MB. 1 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Done. 0:0:1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('')['model_state_dict'])\n",
        "optimizer.load_state_dict(torch.load('')['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "rOGM7PGvhTk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Levenshtein"
      ],
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use debug = True to see debug outputs\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n",
        "    if debug:\n",
        "        pass\n",
        "        # print(f\"\\n----- IN LEVENSHTEIN -----\\n\")\n",
        "        # Add any other debug statements as you may need\n",
        "        # you may want to use debug in several places in this function\n",
        "        \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here\n",
        "    beam_results, _, _, out_lens = decoder.decode(h, seq_lens = lh)\n",
        "    batch_size = y.shape[0] # TODO\n",
        "    distance = 0 # Initialize the distance to be 0 initially\n",
        "\n",
        "    for i in range(batch_size): \n",
        "        # TODO: Loop through each element in the batch\n",
        "        y_seq = y[i, :int(ly[i])]\n",
        "        y_pron = ''.join(labels[int(i)] for i in y_seq)\n",
        "        predict_y_seq = beam_results[i,0, :out_lens[i,0]]\n",
        "        predict_y_pron = ''.join(labels[int(i)] for i in predict_y_seq)\n",
        "        d = Levenshtein.distance(y_pron, predict_y_pron)\n",
        "        distance += d\n",
        "        pass\n",
        "    distance /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    return distance"
      ],
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6fLLj5KIMMOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval function\n",
        "Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."
      ],
      "metadata": {
        "id": "kH0RAbCaMl9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader, model):\n",
        "    model.eval()\n",
        "    dist = 0\n",
        "    loss = 0\n",
        "     batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Val') \n",
        "    val_loss = 0\n",
        "    # TODO Fill this function out, if you're using it.\n",
        "    for i, data in enumerate(data_loader):\n",
        "        # TODO: Fill this with the help of your sanity check\n",
        "        x, y, lx, ly = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        with torch.no_grad():\n",
        "          out, out_lens = model(x, lx)\n",
        "          loss = criterion(out.transpose(0,1), y, out_lens, ly)\n",
        "          d = calculate_levenshtein(out, y, out_lens, ly, decoder, LABELS, debug = False)\n",
        "        val_loss += loss\n",
        "        dist += d\n",
        "        # HINT: Are you using mixed precision? \n",
        "        batch_bar.set_postfix(\n",
        "            loss = f\"{val_loss/ (i+1):.4f}\",\n",
        "            distance = f\"{dist/(i+1):.4f}\"\n",
        "        )\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    val_loss /= len(data_loader)\n",
        "    dist /= len(data_loader) # TODO\n",
        "    return val_loss, dist"
      ],
      "metadata": {
        "id": "0nqLiAmkMMBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Setup"
      ],
      "metadata": {
        "id": "qpYExu4vT4_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config['epochs']\n",
        "best_val_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "dist_freq = 1"
      ],
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, writing a train step might help you code be more modular. You may choose to skip this and write the whole thing out in the training loop below if you so wish."
      ],
      "metadata": {
        "id": "pGn17rLw9ChF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(train_loader, model, optimizer, criterion, scheduler, scaler):\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    train_loss = 0\n",
        "    dist = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # TODO: Fill this with the help of your sanity check\n",
        "        optimizer.zero_grad()\n",
        "        x, y, lx, ly = data\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "          out, out_lens = model(x, lx)\n",
        "          loss = criterion(out.transpose(0,1), y, out_lens, ly)\n",
        "          #d = calculate_levenshtein(out, y, out_lens, ly, decoder, LABELS, debug = False)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # HINT: Are you using mixed precision? \n",
        "        batch_bar.set_postfix(\n",
        "            loss = f\"{train_loss/ (i+1):.4f}\",\n",
        "            #dist = f\"{dist/ (i+1):.4f}\",\n",
        "            lr = f\"{optimizer.param_groups[0]['lr']}\"\n",
        "        )\n",
        "\n",
        "        train_loss += loss\n",
        "        #dist += d\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    train_loss /= len(train_loader) # TODO\n",
        "    #dist /= len(train_loader)\n",
        "    return train_loss#, dist # And anything else you may wish to get out of this function"
      ],
      "metadata": {
        "id": "_vH4QStLUjH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop"
      ],
      "metadata": {
        "id": "MY69hgxUXhTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login(key=\"\") "
      ],
      "metadata": {
        "id": "VVO9TLQr-lIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(\n",
        "    name = config['model'], ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    project = '', entity = '',\n",
        "    config = config)"
      ],
      "metadata": {
        "id": "UdvBYyq38_IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Save your model architecture as a string with str(model) \n",
        "model_arch = str(model)\n",
        "\n",
        "### Save it in a txt file \n",
        "arch_file = open(\"model_arch.txt\", \"w\")\n",
        "file_write = arch_file.write(model_arch)\n",
        "file_write = arch_file.write(\"\\n\")\n",
        "file_write = arch_file.write(\"parameter setting:\\n\")\n",
        "for key, value in config.items(): \n",
        "        arch_file.write('%s:%s\\n' % (key, value))\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ],
      "metadata": {
        "id": "elMnyeUHR2Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypk5H_KNLoPV",
        "outputId": "b0e91434-22a4-46e9-b75f-4ddf2cca2943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "460"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Please complete the training loop\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    # one training step\n",
        "    train_loss = train_step(train_loader, model, optimizer, criterion, scheduler, scaler)\n",
        "    # one validation step (if you want)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    val_loss, val_dist = evaluate(val_loader, model)\n",
        "    scheduler.step(val_loss)\n",
        "    # HINT: Calculating levenshtein distance takes a long time. Do you need to do it every epoch?\n",
        "    # Does the training step even need it? \n",
        "    print(\"\\nEpoch {}/{}: \\t Train Loss {:.04f} \".format(\n",
        "          epoch + 1,\n",
        "          config['epochs'],\n",
        "          train_loss,\n",
        "          #train_dist\n",
        "          ))\n",
        "    print(\"Val dist {:.04f}\\t Val Loss {:.04f}\\t \".format(val_dist, val_loss))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, \n",
        "                'validation_dist':val_dist, 'validation_loss': val_loss,\n",
        "               'lr' : optimizer.param_groups[0]['lr']})\n",
        "    # Where you have your scheduler.step depends on the scheduler you use.\n",
        "\n",
        "    # Use the below code to save models\n",
        "    # if val_dist < best_val_dist:\n",
        "    #   #path = os.path.join(root_path, model_directory, 'checkpoint' + '.pth')\n",
        "    #   print(\"Saving model\")\n",
        "    #   torch.save({'model_state_dict':model.state_dict(),\n",
        "    #               'optimizer_state_dict':optimizer.state_dict(),\n",
        "    #               'val_dist': val_dist, \n",
        "    #               'epoch': epoch}, './checkpoint.pth')\n",
        "    #   best_val_dist = val_dist\n",
        "    #   wandb.save('checkpoint.pth')\n",
        "    if val_dist < best_val_dist:\n",
        "        #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "        print(\"Saving model\")\n",
        "        # Saving the model and optimizer states\n",
        "        torch.save({\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              #'scheduler_state_dict':scheduler.state_dict()\n",
        "              'val_dist': val_dist, \n",
        "               'epoch': epoch\n",
        "              }, \"Model\")\n",
        "        # Creating Artifact\n",
        "        model_artifact = wandb.Artifact(config['model'], type='model')\n",
        "        # Adding model file to Artifact\n",
        "        model_artifact.add_file(\"Model\")\n",
        "        # Saving Artifact to WandB\n",
        "        run.log_artifact(model_artifact)\n",
        "        best_val_dist = val_dist\n",
        "\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "li3kZAHjTFFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_dist = evaluate(train_loader, model)\n",
        "print(train_loss)\n",
        "print(train_dist)"
      ],
      "metadata": {
        "id": "QnpOwk-aGNPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dist"
      ],
      "metadata": {
        "id": "_CnxlktrfmVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ],
      "metadata": {
        "id": "M2H4EEj-sD32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "decoder_test = CTCBeamDecoder(LABELS,\n",
        "                            alpha=0,\n",
        "                            beta=0,\n",
        "                            cutoff_top_n=40,\n",
        "                            cutoff_prob=1.0,\n",
        "                            beam_width=5,\n",
        "                            num_processes=4,\n",
        "                            blank_id=0,\n",
        "                            log_probs_input=True)"
      ],
      "metadata": {
        "id": "2moYJhTWsOG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_loader, model, decoder, LABELS):\n",
        "    model.eval()\n",
        "    df = pd.DataFrame(columns = [\"id\",\"Predicted\"])\n",
        "    predicted = []\n",
        "    for i, data in enumerate(test_loader):\n",
        "        # TODO: Fill this with the help of your sanity check\n",
        "        x, lx = data\n",
        "        x = x.to(device)\n",
        "        # lx = lx.to(device)\n",
        "        with torch.no_grad():\n",
        "          out, out_lens = model(x, lx)\n",
        "        beam_results, beam_scores, timesteps, out_seq_len = decoder_test.decode(out, seq_lens = out_lens) #TODO: What parameters would the decode function take in?\n",
        "        batch_size = out.shape[0]#What is the batch size\n",
        "        for i in range(batch_size): # Loop through each element in the batch\n",
        "          h_sliced = beam_results[i, 0, : out_seq_len[i,0]]#TODO: Obtain the beam results\n",
        "          h_string = ''.join(LABELS[int(i)]for i in h_sliced)#TODO: Convert the beam results to phonemes\n",
        "          predicted.append(h_string)\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "-z57FqOJDamY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:\n",
        "# Write a function (predict) to generate predictions and submit the file to Kaggle\n",
        "torch.cuda.empty_cache()\n",
        "predictions = predict(test_loader, model, decoder, LABELS)"
      ],
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df['index'] = [i for i in range(len(predictions))]\n",
        "df['label'] = predictions\n",
        "df.to_csv('submission.csv', index = False)"
      ],
      "metadata": {
        "id": "Di7Ot3VIeLP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c <competition> -f 'submission.csv' -m \"I made it!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CrCjeQ9bgrA",
        "outputId": "56560fc7-d8c9-4e2e-8e8e-71ebaa245ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: competition: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}