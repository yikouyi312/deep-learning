{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K-R239zlV_QO"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XW--8zIeXIGV"},"outputs":[],"source":["%cd /content/drive/MyDrive/handout\n","%cd /content/drive/MyDrive/handout/hw4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTRL4C_ulUKU"},"outputs":[],"source":["!unzip /content/handout.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1669309388331,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"BKDRz1igl5AH","outputId":"019ca94f-e5bb-4228-eb2d-8b82372d32c1"},"outputs":[],"source":["%cd /content/handout\n","%cd /content/handout/hw4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3280,"status":"ok","timestamp":1669309392741,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"_WVpu1gCV3vO","outputId":"c83a06f9-9a9d-49b4-d2fd-c3f6dd1b8459"},"outputs":[],"source":["! pip install torchsummaryX"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1669319326430,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"oxiZ42B4SwQ-","outputId":"941b2538-44ae-4c43-8a76-3cf90950801d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["%matplotlib inline\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import time\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from tests_hw4 import test_prediction, test_generation\n","from tqdm import tqdm\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wDjDZALxV2fB"},"outputs":[],"source":["from torchsummaryX import summary"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"x5znxQhLSwRC"},"outputs":[],"source":["# load all that we need\n","dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n","devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n","fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n","fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n","fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n","fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n","vocab = np.load('../dataset/vocab.npy')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OZNrJ8XvSwRF"},"outputs":[],"source":["# data loader\n","class DataLoaderForLanguageModeling(DataLoader):\n","    \"\"\"\n","        TODO: Define data loader logic here\n","    \"\"\"\n","    def __init__(self, dataset, batch_size, shuffle=True):\n","        self.dataset = dataset# TODO\n","        self.batch_size = batch_size# TODO\n","        self.shuffle = shuffle# TODO\n","        self.bptt_prob = 0.95\n","        # raise NotImplemented\n","\n","    def __iter__(self):\n","        \"\"\"\n","            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n","            example: Variable length backpropagation sequences (Section 4.1)\n","        \"\"\"\n","        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n","        if self.shuffle:\n","            np.random.shuffle(self.dataset)\n","        # 2. Concatenate all text in one long string.\n","        data = np.concatenate(self.dataset)\n","        # select bptt seq_length for each epoch\n","        seq_p = np.random.random_sample()\n","        if seq_p > self.bptt_prob: \n","            self.seq_len = int(np.random.normal(35, 5))\n","        else: \n","            self.seq_len = int(np.random.normal(70, 5))\n","        batch_number = (len(data)-1) // self.seq_len\n","        # 3. Group the sequences into batches.\n","        x = torch.LongTensor(data[:self.seq_len * batch_number]).reshape(-1, self.seq_len)\n","        y = torch.LongTensor(data[1:self.seq_len * batch_number+1]).reshape(-1, self.seq_len)\n","        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n","        Index = 0\n","        while Index < batch_number:\n","            inputs = x[Index: Index + self.batch_size, :]\n","            targets = y[Index: Index + self.batch_size, :]\n","            Index += self.batch_size\n","            yield(inputs, targets)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"U87BRBtuoqB6"},"outputs":[],"source":["class LockedDropout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, dropout=0.5):\n","        if not self.training or not dropout:\n","            return x\n","        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - dropout)\n","        mask = mask.div_(1 - dropout)\n","        mask = mask.expand_as(x)\n","        return mask * x"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Zt-7YsTYSwRI"},"outputs":[],"source":["# model\n","class Model(nn.Module):\n","    \"\"\"\n","        TODO: Define your model here\n","        https://arxiv.org/abs/1708.02182\n","        https://github.com/salesforce/awd-lstm-lm\n","    \"\"\"\n","    def __init__(self, Dropout, vocab_size:int, embedding_dim:int, hidden_size:int, num_layers:int):\n","        super(Model, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.num_layers = num_layers\n","        self.output_size = vocab_size\n","        self.weight_tying = True\n","        self.hidden_size = hidden_size #if self.weight_tying else 128\n","        #[0.4,0.3,0.4,0.3]\n","        self.dropouti = 0.5\n","        self.dropouth = 0.3\n","        self.dropout = 0.4\n","        self.dropoute = 0.5\n","        self.bidirectional = False\n","\n","        self.lockdropout = Dropout()\n","        self.encode = torch.nn.Embedding(num_embeddings = self.vocab_size, \n","                                         embedding_dim = self.embedding_dim)\n","        # self.rnn = []\n","        # for l in range(self.num_layers):\n","        #     if (l == 0):\n","        #         self.rnn.append(nn.LSTM(input_size = self.embedding_dim, hidden_size = self.hidden_size))\n","        #     # elif self.weight_tying and l == self.num_layers-1:\n","        #     #     self.rnn.append(nn.LSTM(input_size = self.hidden_size, hidden_size = self.embedding_dim))\n","        #     else:\n","        #         self.rnn.append(nn.LSTM(input_size = self.hidden_size, hidden_size = self.hidden_size))\n","        # self.rnn = nn.ModuleList(self.rnn)\n","        self.lstm = nn.LSTM(embedding_dim, self.hidden_size, num_layers= 3,\n","                           batch_first = False)\n","        self.decode = nn.Linear(self.hidden_size, self.output_size)\n","        # raise NotImplemented\n","        # if self.weight_tying:\n","        #     self.encode.weight = self.decode.weight\n","\n","    #     self.init_weights()\n","    #     #initial weights\n","    # def init_weights(self):\n","    #     self.encode.weight.data.uniform_(-0.1, 0.1)\n","    #     self.decode.bias.data.fill_(0)\n","    #     self.decode.weight.data.uniform_(-0.1, 0.1)\n","    \n","    # def embedded_dropout(self, embed, words, dropout=0.1, scale=None):\n","    #     if dropout:\n","    #         mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n","    #         masked_embed_weight = mask * embed.weight\n","    #     else:\n","    #         masked_embed_weight = embed.weight\n","        \n","    #     if scale:\n","    #         masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n","\n","    #     padding_idx = embed.padding_idx\n","    #     if padding_idx is None:\n","    #         padding_idx = -1\n","\n","    #     X = torch.nn.functional.embedding(words, masked_embed_weight, padding_idx, embed.max_norm, \n","    #                                       embed.norm_type, embed.scale_grad_by_freq, embed.sparse)\n","    #     return X\n","    \n","\n","    def forward(self, x):\n","        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n","        x = x.transpose(0,1)\n","        embed = self.encode(x)\n","        embed = self.lockdropout(embed, 0.5)\n","        out, hidden = self.lstm(embed)\n","        out = self.lockdropout(out, 0.5)\n","        out = out.permute(1, 0, 2).contiguous()\n","        out = self.decode(out)\n","        return out\n","        # x = x.transpose(0,1) # Batch x Seq_len ---> Seq_len x Batch, L = sequence length, N = batch size, LxN\n","        # batch_size = x.size(1)\n","        # embed = self.embedded_dropout(self.encode, x, dropout = self.dropoute if self.training else 0)\n","        # if self.training and self.dropouti:\n","        #     embed = self.lockdropout(embed, self.dropouti)\n","        # rnn_inputs = embed # L x N x embedding_size\n","        # rnn_hidden = self.init_hidden(batch_size)\n","        # for l, rnn_layer in enumerate(self.rnn):\n","        #     rnn_outputs, _ = rnn_layer(rnn_inputs, rnn_hidden[l]) #(L, N, Hidden_size)\n","        #     if self.training and self.dropouth and l != self.num_layers-1:\n","        #         rnn_outputs = self.lockdropout(rnn_outputs, self.dropouth)\n","        #     rnn_inputs = rnn_outputs\n","        # raw_outputs = rnn_outputs\n","        # if self.training and self.dropoute:\n","        #     rnn_outputs = self.lockdropout(rnn_outputs, self.dropoute)\n","        # reg_outputs = rnn_outputs\n","        # rnn_outputs = rnn_outputs.permute(1, 0, 2).contiguous() #(N, L, H)\n","        # outputs = self.decode(rnn_outputs) #(N, L, vocab)(B, T, V)\n","        # # outputs = linear_outputs.permute(1, 0, 2).contiguous() #(L, N, vocab)(T, B, V)\n","        # return outputs, raw_outputs, reg_outputs\n","        \n","        # raise NotImplemented"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"kIvZOIfjSwRK"},"outputs":[],"source":["# model trainer\n","class Trainer:\n","    def __init__(self, model, loader, max_epochs=1, run_id='exp',\n","                ):\n","        \"\"\"\n","            Use this class to train your model\n","        \"\"\"\n","        # feel free to add any other parameters here\n","        self.model = model\n","        self.loader = loader\n","        self.train_losses = []\n","        self.val_losses = []\n","        self.predictions = []\n","        self.predictions_test = []\n","        self.generated_logits = []\n","        self.generated = []\n","        self.generated_logits_test = []\n","        self.generated_test = []\n","        self.epochs = 0\n","        self.max_epochs = max_epochs\n","        self.run_id = run_id\n","        \n","        # self.lr = 5.0\n","        # self.weight_decay = 1.2e-6\n","        # TODO: Define your optimizer and criterion here\n","        # feel free to define a learning rate scheduler as well if you want\n","        # self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9)\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.002, weight_decay=1e-5) \n","        self.criterion = torch.nn.CrossEntropyLoss().to(device)\n","        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=3, factor = 0.5)\n","        \n","\n","    def train(self):\n","        self.model.train() # set to training mode\n","        # batch_bar = tqdm(total = len(self.loader), dynamic_ncols = True, leave = False, position = 0, desc = 'Train')\n","        epoch_loss = 0\n","        num_batches = 0\n","        # # Set initial hidden and cell states\n","        for batch_num, (inputs, targets) in enumerate(self.loader):\n","            epoch_loss += self.train_batch(inputs, targets)\n","            # if batch_num % 100 == 0:\n","            #     print('[TRAIN]   Loss: %.4f'\n","            #           % (epoch_loss/(batch_num+1)))\n","        epoch_loss = epoch_loss / (batch_num + 1)\n","        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n","                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n","        self.train_losses.append(epoch_loss)\n","        # self.scheduler.step()\n","\n","    def detach(self, states):\n","        return [state.detach() for state in states]\n","\n","    def train_batch(self, inputs, targets):\n","        \"\"\" \n","            TODO: Define code for training a single batch of inputs\n","            \n","            :return \n","                    (float) loss value\n","        \"\"\"\n","        inputs = inputs.to(device)\n","        targets = targets.to(device) # B, T \n","        out= self.model(inputs) #(B, T, V)\n","        loss = self.criterion(out.view(-1, out.size(2)), targets.view(-1))\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss\n","        # raise NotImplemented\n","\n","    \n","    def test(self):\n","        # don't change these\n","        self.model.eval() # set to eval mode\n","        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n","        self.predictions.append(predictions)\n","        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n","        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n","        nll = test_prediction(predictions, fixtures_pred['out'])\n","        generated = test_generation(fixtures_gen, generated_logits, vocab)\n","        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n","        self.val_losses.append(nll)\n","        \n","        self.generated.append(generated)\n","        self.generated_test.append(generated_test)\n","        self.generated_logits.append(generated_logits)\n","        self.generated_logits_test.append(generated_logits_test)\n","        \n","        # generate predictions for test data\n","        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n","        self.predictions_test.append(predictions_test)\n","            \n","        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n","                      % (self.epochs + 1, self.max_epochs, nll))\n","        self.epochs += 1\n","\n","        return nll\n","\n","    def save(self):\n","        # don't change these\n","        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n","        torch.save({'state_dict': self.model.state_dict()},\n","            model_path)\n","        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n","        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n","        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n","        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n","        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w',  encoding='utf-8') as fw:\n","            fw.write(self.generated[-1])\n","        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w', encoding='utf-8') as fw:\n","            fw.write(self.generated_test[-1])\n","        # fw = codecs.open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w')\n","        # fw.write(self.generated_test[-1])\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xPI7_kZRSwRN"},"outputs":[],"source":["class TestLanguageModel:\n","    def predict(inp, model):\n","        \"\"\"\n","            TODO: write prediction code here\n","            \n","            :param inp:\n","            :return: a np.ndarray of logits\n","        \"\"\"\n","        model.eval()\n","        # Test the model\n","        inp = torch.LongTensor(inp).to(device)\n","        pred= model(inp) # B, T ,vocab\n","        pred = pred.transpose(0,1)[-1] # T, B, vocab---> B vocab\n","        return pred.cpu().detach().numpy()\n","        # raise NotImplemented\n","\n","        \n","    def generate(inp, forward, model):\n","        \"\"\"\n","            TODO: write generation code here\n","\n","            Generate a sequence of words given a starting sequence.\n","            :param inp: Initial sequence of words (batch size, length)\n","            :param forward: number of additional words to generate\n","            :return: generated words (batch size, forward)\n","        \"\"\"\n","        model.eval()\n","        inp = torch.LongTensor(inp).to(device)\n","        with torch.set_grad_enabled(False):\n","            sequence = []\n","            out= model(inp) # B, T, vocab\n","            currword = torch.argmax(out, dim=2)[:,-1].unsqueeze(1) # B, T\n","            sequence.append(currword.cpu().detach())\n","            if forward > 1:\n","                for i in range(forward - 1):\n","                    out= model(currword)\n","                    currword = torch.argmax(out, dim=2)[:,-1].unsqueeze(1) # B, T\n","                    sequence.append(currword.cpu().detach())\n","        return torch.cat(sequence, dim=1).cpu().detach().numpy()\n","        # model.eval()\n","        # inp = torch.LongTensor(inp).to(device)\n","        # with torch.set_grad_enabled(False):\n","        #     sequence = []\n","        #     out, _, _ = model(inp) # B, T, vocab\n","        #     currword = torch.argmax(out, dim=2)#[:,-1].unsqueeze(1) # B, T\n","        #     sequence.append(currword.cpu().detach())\n","        #     if forward > 1:\n","        #         for i in range(forward - 1):\n","        #             out, _, _ = model(currword)\n","        #             currword = torch.argmax(out, dim=2)#[:,-1].unsqueeze(1) # B, T\n","        #             sequence.append(currword.cpu().detach())\n","        # return torch.cat(sequence, dim=1).cpu().detach().numpy()\n","        "]},{"cell_type":"code","execution_count":19,"metadata":{"id":"TiUrjbEjSwRQ"},"outputs":[],"source":["# TODO: define other hyperparameters here\n","\n","NUM_EPOCHS = 80\n","BATCH_SIZE = 80\n","EMB_DIM = 400\n","HIDDEN_SIZE = 1150\n","HIDDEN_LAYERS = 3\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1669323204025,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"2HCVG5YISwRW","outputId":"0fa6d65a-9551-4e15-c40c-413ab6420b9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving models, predictions, and generated words to ./experiments/1669353590\n"]}],"source":["run_id = str(int(time.time()))\n","if not os.path.exists('./experiments'):\n","    os.mkdir('./experiments')\n","os.mkdir('./experiments/%s' % run_id)\n","print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"DbHH6zXTSwRa"},"outputs":[],"source":["model = Model(Dropout = LockedDropout, vocab_size= len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, num_layers=HIDDEN_LAYERS).to(device)\n","# model.apply(init_weights)\n","loader = DataLoaderForLanguageModeling(\n","    dataset=dataset, \n","    batch_size=BATCH_SIZE, \n","    shuffle=True\n",")\n","trainer = Trainer(\n","    model=model, \n","    loader=loader, \n","    max_epochs=NUM_EPOCHS, \n","    run_id=run_id\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"2BVxymCYV2fG"},"outputs":[],"source":["for inputs, target in loader:\n","    break"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":709},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1669323209665,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"hX7X3ZVgV2fG","outputId":"eb8e024f-1e66-443d-fcb7-9826ab3eb8fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["=====================================================================\n","                Kernel Shape     Output Shape      Params Mult-Adds\n","Layer                                                              \n","0_encode        [400, 33278]    [26, 80, 400]    13.3112M  13.3112M\n","1_lockdropout              -    [26, 80, 400]           -         -\n","2_lstm                     -   [26, 80, 1150]    28.3176M    28.29M\n","3_lockdropout              -   [26, 80, 1150]           -         -\n","4_decode       [1150, 33278]  [80, 26, 33278]  38.302978M  38.2697M\n","---------------------------------------------------------------------\n","                          Totals\n","Total params          79.931778M\n","Trainable params      79.931778M\n","Non-trainable params         0.0\n","Mult-Adds               79.8709M\n","=====================================================================\n"]},{"name":"stderr","output_type":"stream","text":["d:\\anaconda3\\lib\\site-packages\\torchsummaryX\\torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n","  df_sum = df.sum()\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_encode</th>\n","      <td>[400, 33278]</td>\n","      <td>[26, 80, 400]</td>\n","      <td>13311200.0</td>\n","      <td>13311200.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_lockdropout</th>\n","      <td>-</td>\n","      <td>[26, 80, 400]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_lstm</th>\n","      <td>-</td>\n","      <td>[26, 80, 1150]</td>\n","      <td>28317600.0</td>\n","      <td>28290000.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_lockdropout</th>\n","      <td>-</td>\n","      <td>[26, 80, 1150]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4_decode</th>\n","      <td>[1150, 33278]</td>\n","      <td>[80, 26, 33278]</td>\n","      <td>38302978.0</td>\n","      <td>38269700.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                Kernel Shape     Output Shape      Params   Mult-Adds\n","Layer                                                                \n","0_encode        [400, 33278]    [26, 80, 400]  13311200.0  13311200.0\n","1_lockdropout              -    [26, 80, 400]         NaN         NaN\n","2_lstm                     -   [26, 80, 1150]  28317600.0  28290000.0\n","3_lockdropout              -   [26, 80, 1150]         NaN         NaN\n","4_decode       [1150, 33278]  [80, 26, 33278]  38302978.0  38269700.0"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["summary(model,inputs.to(device))"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1669323211934,"user":{"displayName":"Shuxian Xu","userId":"05191082119610314637"},"user_tz":300},"id":"rUSSDlqiV2fG","outputId":"83364fe8-0754-42d9-bc95-11c35924909b"},"outputs":[{"data":{"text/plain":["1347"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7D8wTJkBSwRc","outputId":"0b03f796-22f6-4ad5-eb76-db99411aba3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[TRAIN]  Epoch [1/80]   Loss: 6.7914\n","0.002\n","[VAL]  Epoch [1/80]   Loss: 5.5629\n","Saving model, predictions and generated output for epoch 0 with NLL: 5.5628934\n","[TRAIN]  Epoch [2/80]   Loss: 6.0443\n","0.002\n","[VAL]  Epoch [2/80]   Loss: 5.1450\n","Saving model, predictions and generated output for epoch 1 with NLL: 5.1450415\n","[TRAIN]  Epoch [3/80]   Loss: 5.8123\n","0.002\n","[VAL]  Epoch [3/80]   Loss: 4.8758\n","Saving model, predictions and generated output for epoch 2 with NLL: 4.8758135\n","[TRAIN]  Epoch [4/80]   Loss: 5.6004\n","0.002\n","[VAL]  Epoch [4/80]   Loss: 4.8060\n","Saving model, predictions and generated output for epoch 3 with NLL: 4.80601\n","[TRAIN]  Epoch [5/80]   Loss: 5.4825\n","0.002\n","[VAL]  Epoch [5/80]   Loss: 4.6623\n","Saving model, predictions and generated output for epoch 4 with NLL: 4.6623487\n","[TRAIN]  Epoch [6/80]   Loss: 5.3694\n","0.002\n","[VAL]  Epoch [6/80]   Loss: 4.6575\n","Saving model, predictions and generated output for epoch 5 with NLL: 4.6574974\n","[TRAIN]  Epoch [7/80]   Loss: 5.2597\n","0.002\n","[VAL]  Epoch [7/80]   Loss: 4.5824\n","Saving model, predictions and generated output for epoch 6 with NLL: 4.5824127\n","[TRAIN]  Epoch [8/80]   Loss: 5.2135\n","0.002\n","[VAL]  Epoch [8/80]   Loss: 4.5552\n","Saving model, predictions and generated output for epoch 7 with NLL: 4.55516\n","[TRAIN]  Epoch [9/80]   Loss: 5.2491\n","0.002\n","[VAL]  Epoch [9/80]   Loss: 4.5404\n","Saving model, predictions and generated output for epoch 8 with NLL: 4.540398\n","[TRAIN]  Epoch [10/80]   Loss: 5.0308\n","0.002\n","[VAL]  Epoch [10/80]   Loss: 4.4145\n","Saving model, predictions and generated output for epoch 9 with NLL: 4.4144864\n","[TRAIN]  Epoch [11/80]   Loss: 5.0169\n","0.002\n","[VAL]  Epoch [11/80]   Loss: 4.3486\n","Saving model, predictions and generated output for epoch 10 with NLL: 4.3486433\n","[TRAIN]  Epoch [12/80]   Loss: 4.9475\n","0.002\n","[VAL]  Epoch [12/80]   Loss: 4.4364\n","[TRAIN]  Epoch [13/80]   Loss: 4.9152\n","0.002\n","[VAL]  Epoch [13/80]   Loss: 4.3994\n","[TRAIN]  Epoch [14/80]   Loss: 4.8605\n","0.002\n","[VAL]  Epoch [14/80]   Loss: 4.4196\n","[TRAIN]  Epoch [15/80]   Loss: 4.8716\n","0.002\n","[VAL]  Epoch [15/80]   Loss: 4.3333\n","Saving model, predictions and generated output for epoch 14 with NLL: 4.333272\n","[TRAIN]  Epoch [16/80]   Loss: 4.8424\n","0.002\n","[VAL]  Epoch [16/80]   Loss: 4.3460\n","[TRAIN]  Epoch [17/80]   Loss: 4.8257\n","0.002\n","[VAL]  Epoch [17/80]   Loss: 4.4352\n","[TRAIN]  Epoch [18/80]   Loss: 4.7384\n","0.002\n","[VAL]  Epoch [18/80]   Loss: 4.3203\n","Saving model, predictions and generated output for epoch 17 with NLL: 4.3203306\n","[TRAIN]  Epoch [19/80]   Loss: 4.7369\n","0.002\n","[VAL]  Epoch [19/80]   Loss: 4.4210\n","[TRAIN]  Epoch [20/80]   Loss: 4.7271\n","0.002\n","[VAL]  Epoch [20/80]   Loss: 4.3021\n","Saving model, predictions and generated output for epoch 19 with NLL: 4.302102\n","[TRAIN]  Epoch [21/80]   Loss: 4.7011\n","0.002\n","[VAL]  Epoch [21/80]   Loss: 4.2990\n","Saving model, predictions and generated output for epoch 20 with NLL: 4.299022\n","[TRAIN]  Epoch [22/80]   Loss: 4.6989\n","0.002\n","[VAL]  Epoch [22/80]   Loss: 4.3110\n","[TRAIN]  Epoch [23/80]   Loss: 4.6834\n","0.002\n","[VAL]  Epoch [23/80]   Loss: 4.3403\n"]}],"source":["best_nll = 1e30 \n","for epoch in range(NUM_EPOCHS):\n","    trainer.train()\n","    print(trainer.optimizer.param_groups[0]['lr'])\n","    nll = trainer.test()\n","    trainer.scheduler.step(nll)\n","    if nll < best_nll:\n","        best_nll = nll\n","        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n","        trainer.save()\n","    "]},{"cell_type":"markdown","metadata":{"id":"z2KTyxWdNidS"},"source":["1669331706 #28  5.7986/4.399625\n","\n","1669339744  #34,  [0.4,0.3,0.4,0.3]\n","\n","1669348804  #     [0.5,0.3,0.4,0.5]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"executionInfo":{"elapsed":419,"status":"error","timestamp":1669263725446,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":300},"id":"z2FmDqBCSwRf","outputId":"c64d6b2b-ffd7-4b38-ef3b-14a645fd3404"},"outputs":[],"source":["# Don't change these\n","# plot training curves\n","plt.figure()\n","plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n","plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n","plt.xlabel('Epochs')\n","plt.ylabel('NLL')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1669263732998,"user":{"displayName":"Shuxian Xu","userId":"02245261237530543008"},"user_tz":300},"id":"ipdbmqaGSwRh","outputId":"d2d10311-0dcb-43c2-8ec2-c0315e083e62"},"outputs":[],"source":["# see generated output\n","print (trainer.generated[-1]) # get last generated output"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"}}},"nbformat":4,"nbformat_minor":0}
