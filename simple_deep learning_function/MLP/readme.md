## Table of contents
* [MLP](#MLP) 
* [autograde](#autograde)

# MLP
  - Activation Functions
    - Sigmoid `MLP\mytorch.nn.Sigmoid`
    - Tanh `MLP\mytorch.nn.Tanh`
    - ReLU `MLP\mytorch.nn.ReLU`
  - Loss Functions
    - MSE Loss `MLP\mytorch.nn.MSELoss`
    - Cross-Entropy Loss `MLP\mytorch.nn.CrossEntropyLoss`
  - Linear Layer `MLP\mytorch.nn.Linear`
  - Optimizers `MLP\mytorch.optim.SGD`
  - Regularization
    - Batch Normalization `mytorch.nn.BatchNorm1d`  
    
# autograde
  - Verify the code
    - `MLP\autograder.ipynb`
    - `MLP\autograder.py`
    - `MLP\testsgd.py`
