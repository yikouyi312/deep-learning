## Automatic Differentiation
* a framework that allows us to calculate the derivatives of any arbitrarily complex mathematical function.
  - forward accumulation, computes the derivatives of the chain rule from inside to outside
  - reverse accumulation, computes the derivatives of the chain rule from outside to inside
*  Autograd framework keeps track of the sequence of operations that are performed on the input data leading up to the final loss calculation. It then performs backpropagation and calculates all the necessary gradients.
## Table of contents
* [AutogradEngine classes]
* [MemoryBuffer classes]
* [Operation classes]
* [Other function]


# AutogradEngine
* `mytorch/autograd_engine.py`
* Class attributes:
  - memory buffer : An instance of the MemoryBuffer class, used to store the gradients.
  - operation list : A Python list that is used to store sequence of operations that are performed on the input data. Concretely, this stores Operation objects.
* Class methods:
  - add node : Initialises and adds an instance of the Operation class to the operations list.
  - backward : Kicks off backprop. Traverses the operations list in reverse and calculates the gradients at every Node.

# MemoryBuffer
* `mytorch/utils.py`
* Class attributes:
  - memory: A Python dictionary that holds the NumPy arrays corresponding to the gradients of the network. The key is the memory location of the NumPy array and the value is the actual array. 
    - Note : Using the memory location as a key is a simple trick that eliminates the need to perform extra bookkeeping of maintaining unique keys for all gradients.
* Class methods:
  - get memory loc: Returns the corresponding parameter array for the input array.
  - is in memory: Checks if an array is already provided for in memory.
  - add spot: Initialises a location in memory for a new array.
  - update param: Updates the parameter in memory by adding a new update to it.
  - get param: Returns the appropriate param from memory dict.
  - clear: Resets the memory dictionary to effectively zero out all gradients.
 
 # Operation
 * Class attributes:
  - inputs : The inputs to the operation, for our implementation, we refer to the input data, and network parameters as inputs to the Operation.
  - outputs : The output(s) generated by applying the operation to the inputs.
  - gradients to update : These are the gradients corresponding to the inputs that must be updated on the backward pass.
  - backward function : A backward function implemented for a specific operation.
    - `mytorch/nn/functional.py`

  # Other functions
  * Activations
    - `mytorch/nn/modules/activations.py`
  * Loss Function 
    -  `mytorch/nn/modules/loss.py`
